{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BLUE BIKE TRIP DURATION PREDICTION"
      ],
      "metadata": {
        "id": "U8YOCpyn-rKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection"
      ],
      "metadata": {
        "id": "aGmWbnX4sAt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Dropped features**: First of all, I decided to drop 'bikeid' since it doesn't add more explanation to the data. I also dropped 'start station name' and 'end station name' to avoid duplicates because there are 'station id', which essentially indicate station name. Lastly, I removed 'stoptime' to avoid cheating for prediction.\n",
        "\n",
        "\n",
        "* **Kept features**: First of all, I kept 'tripduration' because it is target variable. Next, I kept 'starttime', 'station id', 'latitude' and 'longitude' of start and end station because time, the location of start and end station, and distance between stations might affect trip duration. I also decided to have 'usertype' since subscriber and onetime customer might have different trip duration based on their pass. Lastly, I kept 'birth year' and 'gender' since these featurs might indicate user characteristic such as physical ability for trip duration."
      ],
      "metadata": {
        "id": "nMnSfAK-sKu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "kFXct052sUL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Starttime to minutes and seconds**: Since it was in the format of string and consist of minutes and seconds, I parsed it by\":\", expanded it to new columns - 'start_min' and 'start_sec', which indicate minutes and seconds respectively, and converted those to integer and float. After that, I dropped the original column 'starttime'\n",
        "\n",
        "* **Birthyear to Age**: I converted birthyear to age by subtracting age from 2022, to better understand the impact of age to target variable. Then I dropped 'birthyear' column.\n",
        "\n",
        "* **Usertype**: I transformed usertype to dummy variables and dropped the first feature to avoid duplicates.\n",
        "\n",
        "* **Outliers in trip duration**: I examined data to see whether it is balanced. I figured out there are some outliers in trip duration and the distribution is right skewed. Based on Bluebikes policy, single trip pass includes 30 minutes rides and membership offers 45 minutes ride. After that, users should pay $2.50 per additional 30 minutes. Accordingly, most users will trip less than 60 minutes. In addition, too long trips might result from lost or unlocked bicycles. Therefore, I dropped 118 rows which are larger than 3600(=60 minutes) in trip duration, accounting for less than 1.5% of the total number of data."
      ],
      "metadata": {
        "id": "LLNOOeulsd5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN model configuration"
      ],
      "metadata": {
        "id": "3cOAuTsIse7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the output layer, I chose 'linear' activation function and ruled out 'sigmoid' and 'softmax' activation functions.\n",
        "\n",
        "* This case is **regression problem** which predicts trip duration, a numerical variable. Therefore, I considered **linear activation function** because it is identity Function where the activation is proportional to the input and all layers of the neural network will collapse into one layer.\n",
        "\n",
        "* I excluded **sigmoid function** because it is non linear and for binary or multilabel classification problem. Sigmoid activation function in the output layer have the values 0 or 1, which is not appropriate for this case.\n",
        "\n",
        "* I ruled out **softmax function** because it is non linear and better used for multiclass classificaton. Softmax function outputs a vector of values between 0 and 1 that can be interpreted as probabilities."
      ],
      "metadata": {
        "id": "0VqmuDQ2sid-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness of model's performance"
      ],
      "metadata": {
        "id": "epbB9Es8srBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Regularizing Weights**: I added 'kernel_regularizer=\"l2\"' in the input layer to add a penalty for weight size to the loss function and thus avoid overfitting.\n",
        "\n",
        "* **Batch Normalization**: I included BatchNormalization to standardize the inputs to a layer for each mini-batch, by de-mean and division by SD. This whites the inputs to the next layer and help with training.\n",
        "\n",
        "* **Drop out**: I added drop out by 10% in the hidden layer to regularize and avoid overfitting, by making the network become less sensitive to the specific weights of neurons.\n",
        "\n",
        "* **Batch size**: I increased batch size from 16 and settled down at 128. Since the data is not large, batch size larger than 128 caused overfitting. \n",
        "\n",
        "* **Number of nodes**: I also increased the number of nodes in layer to 64, followed by 32 to increase complexity of the model and avoid underfitting.\n",
        "\n",
        "* **Cross validation for compatibility**: I conducted 5 folds cross validation to ensure compatibility with new samples of data. I also chose the number of epochs as 100 by looking at validation loss curve."
      ],
      "metadata": {
        "id": "3ZGbcpYis1S8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Neural Network"
      ],
      "metadata": {
        "id": "BeNdB8Y7tNLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Import and Pre-process Data*"
      ],
      "metadata": {
        "id": "jF365-n0EC-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# No need to change this; this is the dataset I'm providing to you for training.\n",
        "bluebikes = pd.read_csv('https://raw.githubusercontent.com/gburtch/BA865-2022/main/Week%203/datasets/bluebikes_sample.csv')\n"
      ],
      "metadata": {
        "id": "Uw19ViYpDoL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There isn't any missing values\n",
        "bluebikes.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to46FbIBD8yL",
        "outputId": "8cb05ee2-5754-4d7c-f3e6-b85dd180dd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tripduration               0\n",
              "starttime                  0\n",
              "stoptime                   0\n",
              "start station id           0\n",
              "start station name         0\n",
              "start station latitude     0\n",
              "start station longitude    0\n",
              "end station id             0\n",
              "end station name           0\n",
              "end station latitude       0\n",
              "end station longitude      0\n",
              "bikeid                     0\n",
              "usertype                   0\n",
              "birth year                 0\n",
              "gender                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Some columns are ojbect, which need to be pre-processed\n",
        "bluebikes.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em9kMaRHKODb",
        "outputId": "ed9c11db-c1b3-42d2-d2a9-14b82c195f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9000 entries, 0 to 8999\n",
            "Data columns (total 15 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   tripduration             9000 non-null   int64  \n",
            " 1   starttime                9000 non-null   object \n",
            " 2   stoptime                 9000 non-null   object \n",
            " 3   start station id         9000 non-null   int64  \n",
            " 4   start station name       9000 non-null   object \n",
            " 5   start station latitude   9000 non-null   float64\n",
            " 6   start station longitude  9000 non-null   float64\n",
            " 7   end station id           9000 non-null   int64  \n",
            " 8   end station name         9000 non-null   object \n",
            " 9   end station latitude     9000 non-null   float64\n",
            " 10  end station longitude    9000 non-null   float64\n",
            " 11  bikeid                   9000 non-null   int64  \n",
            " 12  usertype                 9000 non-null   object \n",
            " 13  birth year               9000 non-null   int64  \n",
            " 14  gender                   9000 non-null   int64  \n",
            "dtypes: float64(4), int64(6), object(5)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution and outliers of columns\n",
        "bluebikes.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "TZKQV6AfKR15",
        "outputId": "632350ef-7297-43e0-942f-194e42ade4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1c87c296-e60d-4064-b1d0-12e81dbd68af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tripduration</th>\n",
              "      <th>start station id</th>\n",
              "      <th>start station latitude</th>\n",
              "      <th>start station longitude</th>\n",
              "      <th>end station id</th>\n",
              "      <th>end station latitude</th>\n",
              "      <th>end station longitude</th>\n",
              "      <th>bikeid</th>\n",
              "      <th>birth year</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "      <td>9000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>849.892444</td>\n",
              "      <td>154.729444</td>\n",
              "      <td>42.357836</td>\n",
              "      <td>-71.089593</td>\n",
              "      <td>151.271333</td>\n",
              "      <td>42.357827</td>\n",
              "      <td>-71.088978</td>\n",
              "      <td>4233.924000</td>\n",
              "      <td>1985.960889</td>\n",
              "      <td>1.159556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1398.751895</td>\n",
              "      <td>128.375671</td>\n",
              "      <td>0.016471</td>\n",
              "      <td>0.024170</td>\n",
              "      <td>126.655490</td>\n",
              "      <td>0.016443</td>\n",
              "      <td>0.024013</td>\n",
              "      <td>1265.847251</td>\n",
              "      <td>11.288104</td>\n",
              "      <td>0.531994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>42.280725</td>\n",
              "      <td>-71.166491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>42.267902</td>\n",
              "      <td>-71.166491</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>1900.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>380.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>42.348706</td>\n",
              "      <td>-71.105301</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>42.348717</td>\n",
              "      <td>-71.104412</td>\n",
              "      <td>3184.000000</td>\n",
              "      <td>1980.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>623.500000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>42.358100</td>\n",
              "      <td>-71.090179</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>42.358100</td>\n",
              "      <td>-71.089811</td>\n",
              "      <td>4351.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1023.000000</td>\n",
              "      <td>221.000000</td>\n",
              "      <td>42.366277</td>\n",
              "      <td>-71.071190</td>\n",
              "      <td>206.000000</td>\n",
              "      <td>42.365994</td>\n",
              "      <td>-71.069957</td>\n",
              "      <td>5368.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>86912.000000</td>\n",
              "      <td>446.000000</td>\n",
              "      <td>42.412505</td>\n",
              "      <td>-71.016191</td>\n",
              "      <td>446.000000</td>\n",
              "      <td>42.414273</td>\n",
              "      <td>-71.006098</td>\n",
              "      <td>6173.000000</td>\n",
              "      <td>2002.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c87c296-e60d-4064-b1d0-12e81dbd68af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c87c296-e60d-4064-b1d0-12e81dbd68af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c87c296-e60d-4064-b1d0-12e81dbd68af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       tripduration  start station id  ...   birth year       gender\n",
              "count   9000.000000       9000.000000  ...  9000.000000  9000.000000\n",
              "mean     849.892444        154.729444  ...  1985.960889     1.159556\n",
              "std     1398.751895        128.375671  ...    11.288104     0.531994\n",
              "min       61.000000          1.000000  ...  1900.000000     0.000000\n",
              "25%      380.000000         55.000000  ...  1980.000000     1.000000\n",
              "50%      623.500000        107.000000  ...  1989.000000     1.000000\n",
              "75%     1023.000000        221.000000  ...  1994.000000     1.000000\n",
              "max    86912.000000        446.000000  ...  2002.000000     2.000000\n",
              "\n",
              "[8 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of trip duration\n",
        "* Trip duration is right skewed.\n",
        "* I dropped 118 outliers which are larger than 60 minutes in trip duration.\n",
        "* It accounts for less than 1.5% of the total number of data."
      ],
      "metadata": {
        "id": "vVMTZlR3uR2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking outliers and distribution of trip duration\n",
        "sns.histplot(data=bluebikes, x=\"tripduration\",kde=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "kYyhIKE2Mmzr",
        "outputId": "161d42c9-490d-4415-af6f-55ae3ec10c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f752dc44990>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I will drop trips longer than 60 minutes, total 118 rows.\n",
        "len(bluebikes[bluebikes['tripduration'] > 3600])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPH_TFjoEyjD",
        "outputId": "e55f3c43-f554-4c91-ffcf-d65b92c3fb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Data\n",
        "* Drop outliers\n",
        "* Convert birthyear to age\n",
        "* Parse starttime\n",
        "* Get dummy variables\n",
        "* Drop unnecessary columns"
      ],
      "metadata": {
        "id": "4sBH-65_u0V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def processData(data):\n",
        "    Data = data\n",
        "    # dropping outliers\n",
        "    Data.drop(Data[Data['tripduration'] > 3600].index, inplace = True)\n",
        "    # converting birthyear to age\n",
        "    Data['age'] = 2022 - Data['birth year']\n",
        "    # process starttime to min and sec\n",
        "    Data[['start_min','start_sec']] = Data.starttime.str.split(pat=':',expand=True)\n",
        "    Data['start_min'] = pd.to_numeric(Data['start_min'])\n",
        "    Data['start_sec'] = pd.to_numeric(Data['start_sec'])\n",
        "    # dropping unnecessary columns\n",
        "    Data.drop(columns=['birth year','start station name', 'end station name', 'bikeid'], axis=1, inplace=True)\n",
        "    Data.drop(['starttime', 'stoptime'], axis=1, inplace=True)\n",
        "    # getting dummy variables for usertype\n",
        "    Data = pd.get_dummies(Data, columns=['usertype'], drop_first=True)\n",
        "\n",
        "    # Make sure you send back a pair of numpy arrays that can then feed into your NN model.\n",
        "    predictors = Data.drop(columns=['tripduration'], axis=1)\n",
        "    labels = Data['tripduration']\n",
        "\n",
        "    return predictors, labels\n"
      ],
      "metadata": {
        "id": "a2ab2j0wz3oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Specify Neural Network Architecture, Process Your Sample*"
      ],
      "metadata": {
        "id": "vhnwnEtEOeXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calling the data pre-processing function on the dataset."
      ],
      "metadata": {
        "id": "8KZXFE1fWMcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictors, labels = processData(bluebikes)"
      ],
      "metadata": {
        "id": "HToKOpiSoZmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zvAohtIHJi0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split"
      ],
      "metadata": {
        "id": "gaM6ysZ7vFQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors, labels, test_size=0.2, random_state=865)"
      ],
      "metadata": {
        "id": "NNh29n2PHVOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "eTP9bd6RJpzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Neural Network Model\n",
        "* Model loss : MAE\n",
        "* Optimizer : Adam\n",
        "* 3 hidden layers\n"
      ],
      "metadata": {
        "id": "RGYaiNdpoako"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(64, activation=\"relu\",kernel_regularizer=\"l2\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.1),\n",
        "        layers.Dense(32, activation=\"selu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(8, activation=\"selu\"),\n",
        "        layers.Dense(1, activation=\"linear\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mae\"])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8JBFs2OXdZzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "fVXCT9SAffgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross- Validation by 5 folds\n",
        "k = 5\n",
        "num_val_samples = len(X_train) // k \n",
        "num_epochs = 150\n",
        "all_loss_history = []\n",
        "all_val_loss_history = []\n",
        "\n",
        "print(\"In total, we have\",len(X_train),\"training observations.\")\n",
        "print(\"With a k of\",k,\"we have\",num_val_samples,\"observations per fold.\\n\")\n",
        "\n",
        "for i in range(k): \n",
        "    print(\"Processing fold #:\",i)\n",
        " \n",
        "    print(\"Validation data includes observations\",i*num_val_samples,\"through\",(i+1)*num_val_samples-1) \n",
        "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples] \n",
        "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    print(\"Training data includes observations 0 through\",i*num_val_samples-1,\"joined with observations\",(i+1)*num_val_samples,\"through the final observation.\\n\")\n",
        "    partial_train_data = np.concatenate(\n",
        "        [X_train[:i * num_val_samples],\n",
        "         X_train[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [y_train[:i * num_val_samples],\n",
        "         y_train[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    model = build_model()\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=128, verbose=1)\n",
        "    val_loss_history = history.history['val_loss']\n",
        "    loss_history = history.history['loss']\n",
        "    all_val_loss_history.append(val_loss_history)\n",
        "    all_loss_history.append(loss_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjrdXHEN85kG",
        "outputId": "9bdc1e3a-3b8f-4265-eada-9f5ad4c28a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In total, we have 7105 training observations.\n",
            "With a k of 5 we have 1421 observations per fold.\n",
            "\n",
            "Processing fold #: 0\n",
            "Validation data includes observations 0 through 1420\n",
            "Training data includes observations 0 through -1 joined with observations 1421 through the final observation.\n",
            "\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - 2s 12ms/step - loss: 752.8421 - mae: 752.6581 - val_loss: 752.1848 - val_mae: 752.0051\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 743.8387 - mae: 743.6642 - val_loss: 709.2703 - val_mae: 709.1010\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 721.9767 - mae: 721.8127 - val_loss: 636.5574 - val_mae: 636.3991\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 671.5095 - mae: 671.3548 - val_loss: 549.0931 - val_mae: 548.9416\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 586.4895 - mae: 586.3399 - val_loss: 485.6819 - val_mae: 485.5341\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 487.1778 - mae: 487.0305 - val_loss: 401.0358 - val_mae: 400.8885\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 418.7702 - mae: 418.6223 - val_loss: 425.2566 - val_mae: 425.1085\n",
            "Epoch 8/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 390.5453 - mae: 390.3965 - val_loss: 454.7692 - val_mae: 454.6200\n",
            "Epoch 9/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 379.2897 - mae: 379.1393 - val_loss: 415.0377 - val_mae: 414.8863\n",
            "Epoch 10/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 370.5212 - mae: 370.3681 - val_loss: 420.9735 - val_mae: 420.8195\n",
            "Epoch 11/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 345.5705 - mae: 345.4160 - val_loss: 420.9484 - val_mae: 420.7934\n",
            "Epoch 12/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 313.4664 - mae: 313.3111 - val_loss: 500.1195 - val_mae: 499.9641\n",
            "Epoch 13/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 300.3969 - mae: 300.2415 - val_loss: 540.4832 - val_mae: 540.3273\n",
            "Epoch 14/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 281.5729 - mae: 281.4165 - val_loss: 495.2887 - val_mae: 495.1320\n",
            "Epoch 15/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 269.4057 - mae: 269.2483 - val_loss: 364.5197 - val_mae: 364.3621\n",
            "Epoch 16/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 258.3308 - mae: 258.1732 - val_loss: 306.8539 - val_mae: 306.6962\n",
            "Epoch 17/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 256.5936 - mae: 256.4354 - val_loss: 264.2686 - val_mae: 264.1100\n",
            "Epoch 18/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 257.1284 - mae: 256.9698 - val_loss: 247.7905 - val_mae: 247.6317\n",
            "Epoch 19/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 255.3702 - mae: 255.2113 - val_loss: 236.1243 - val_mae: 235.9653\n",
            "Epoch 20/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 254.2640 - mae: 254.1045 - val_loss: 237.9835 - val_mae: 237.8237\n",
            "Epoch 21/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 256.4446 - mae: 256.2847 - val_loss: 231.5830 - val_mae: 231.4231\n",
            "Epoch 22/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 251.4678 - mae: 251.3078 - val_loss: 228.1492 - val_mae: 227.9891\n",
            "Epoch 23/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 253.5267 - mae: 253.3666 - val_loss: 225.6373 - val_mae: 225.4771\n",
            "Epoch 24/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 250.5148 - mae: 250.3546 - val_loss: 226.2722 - val_mae: 226.1120\n",
            "Epoch 25/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 252.8784 - mae: 252.7181 - val_loss: 223.5382 - val_mae: 223.3777\n",
            "Epoch 26/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 249.8696 - mae: 249.7090 - val_loss: 223.9030 - val_mae: 223.7419\n",
            "Epoch 27/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 247.1845 - mae: 247.0231 - val_loss: 224.5779 - val_mae: 224.4158\n",
            "Epoch 28/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 246.6962 - mae: 246.5341 - val_loss: 225.8624 - val_mae: 225.7004\n",
            "Epoch 29/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 246.7737 - mae: 246.6116 - val_loss: 226.4207 - val_mae: 226.2582\n",
            "Epoch 30/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 244.8447 - mae: 244.6820 - val_loss: 222.5210 - val_mae: 222.3582\n",
            "Epoch 31/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 243.2986 - mae: 243.1358 - val_loss: 221.0359 - val_mae: 220.8729\n",
            "Epoch 32/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 240.2254 - mae: 240.0625 - val_loss: 221.3946 - val_mae: 221.2317\n",
            "Epoch 33/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 240.1955 - mae: 240.0324 - val_loss: 219.3307 - val_mae: 219.1675\n",
            "Epoch 34/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 243.6826 - mae: 243.5189 - val_loss: 214.6211 - val_mae: 214.4575\n",
            "Epoch 35/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 236.0203 - mae: 235.8565 - val_loss: 210.1173 - val_mae: 209.9532\n",
            "Epoch 36/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 241.3810 - mae: 241.2167 - val_loss: 207.2287 - val_mae: 207.0644\n",
            "Epoch 37/150\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 234.0015 - mae: 233.8370 - val_loss: 208.7999 - val_mae: 208.6356\n",
            "Epoch 38/150\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 234.2299 - mae: 234.0655 - val_loss: 207.6292 - val_mae: 207.4647\n",
            "Epoch 39/150\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 233.2912 - mae: 233.1265 - val_loss: 205.5972 - val_mae: 205.4324\n",
            "Epoch 40/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 231.9899 - mae: 231.8248 - val_loss: 203.4783 - val_mae: 203.3130\n",
            "Epoch 41/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 230.8725 - mae: 230.7068 - val_loss: 202.9143 - val_mae: 202.7486\n",
            "Epoch 42/150\n",
            "45/45 [==============================] - 0s 6ms/step - loss: 227.9109 - mae: 227.7451 - val_loss: 205.5835 - val_mae: 205.4175\n",
            "Epoch 43/150\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 230.2408 - mae: 230.0748 - val_loss: 201.3488 - val_mae: 201.1826\n",
            "Epoch 44/150\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 227.7394 - mae: 227.5731 - val_loss: 202.0387 - val_mae: 201.8723\n",
            "Epoch 45/150\n",
            "45/45 [==============================] - 0s 6ms/step - loss: 229.3333 - mae: 229.1666 - val_loss: 202.4002 - val_mae: 202.2332\n",
            "Epoch 46/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.8126 - mae: 224.6454 - val_loss: 199.1898 - val_mae: 199.0222\n",
            "Epoch 47/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 226.8998 - mae: 226.7320 - val_loss: 199.3589 - val_mae: 199.1909\n",
            "Epoch 48/150\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 226.2393 - mae: 226.0709 - val_loss: 201.3685 - val_mae: 201.2001\n",
            "Epoch 49/150\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 229.7249 - mae: 229.5564 - val_loss: 201.4043 - val_mae: 201.2356\n",
            "Epoch 50/150\n",
            "45/45 [==============================] - 0s 7ms/step - loss: 223.8334 - mae: 223.6647 - val_loss: 202.5668 - val_mae: 202.3980\n",
            "Epoch 51/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 225.6497 - mae: 225.4810 - val_loss: 205.8450 - val_mae: 205.6762\n",
            "Epoch 52/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 225.9942 - mae: 225.8252 - val_loss: 199.6163 - val_mae: 199.4468\n",
            "Epoch 53/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.0988 - mae: 223.9291 - val_loss: 199.0841 - val_mae: 198.9143\n",
            "Epoch 54/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.3259 - mae: 224.1559 - val_loss: 199.6046 - val_mae: 199.4348\n",
            "Epoch 55/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 223.2489 - mae: 223.0789 - val_loss: 198.2527 - val_mae: 198.0826\n",
            "Epoch 56/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.2160 - mae: 221.0457 - val_loss: 199.7887 - val_mae: 199.6183\n",
            "Epoch 57/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.2708 - mae: 222.1003 - val_loss: 202.4956 - val_mae: 202.3251\n",
            "Epoch 58/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.9911 - mae: 222.8205 - val_loss: 204.3605 - val_mae: 204.1899\n",
            "Epoch 59/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.6523 - mae: 223.4817 - val_loss: 198.1228 - val_mae: 197.9520\n",
            "Epoch 60/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.2473 - mae: 222.0764 - val_loss: 204.4900 - val_mae: 204.3190\n",
            "Epoch 61/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.9995 - mae: 220.8284 - val_loss: 200.1208 - val_mae: 199.9495\n",
            "Epoch 62/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.4282 - mae: 222.2567 - val_loss: 200.5692 - val_mae: 200.3975\n",
            "Epoch 63/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.0471 - mae: 219.8755 - val_loss: 199.9207 - val_mae: 199.7490\n",
            "Epoch 64/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 223.7306 - mae: 223.5587 - val_loss: 199.6931 - val_mae: 199.5210\n",
            "Epoch 65/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.0435 - mae: 223.8712 - val_loss: 199.9336 - val_mae: 199.7612\n",
            "Epoch 66/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.9737 - mae: 218.8014 - val_loss: 200.0210 - val_mae: 199.8486\n",
            "Epoch 67/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 223.7995 - mae: 223.6271 - val_loss: 201.8865 - val_mae: 201.7139\n",
            "Epoch 68/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.6665 - mae: 221.4937 - val_loss: 202.6419 - val_mae: 202.4691\n",
            "Epoch 69/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.4552 - mae: 221.2822 - val_loss: 203.0859 - val_mae: 202.9125\n",
            "Epoch 70/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.3483 - mae: 222.1750 - val_loss: 201.0703 - val_mae: 200.8969\n",
            "Epoch 71/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.8058 - mae: 219.6324 - val_loss: 200.5592 - val_mae: 200.3857\n",
            "Epoch 72/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.0874 - mae: 218.9136 - val_loss: 202.7025 - val_mae: 202.5287\n",
            "Epoch 73/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.3203 - mae: 225.1460 - val_loss: 206.2735 - val_mae: 206.0992\n",
            "Epoch 74/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.1448 - mae: 219.9703 - val_loss: 202.9174 - val_mae: 202.7427\n",
            "Epoch 75/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.5180 - mae: 219.3431 - val_loss: 200.8650 - val_mae: 200.6898\n",
            "Epoch 76/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.1400 - mae: 219.9650 - val_loss: 199.8779 - val_mae: 199.7032\n",
            "Epoch 77/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.7717 - mae: 219.5968 - val_loss: 204.0495 - val_mae: 203.8744\n",
            "Epoch 78/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.5393 - mae: 221.3642 - val_loss: 199.3767 - val_mae: 199.2015\n",
            "Epoch 79/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.3701 - mae: 220.1948 - val_loss: 202.7575 - val_mae: 202.5817\n",
            "Epoch 80/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.2142 - mae: 221.0384 - val_loss: 200.4116 - val_mae: 200.2357\n",
            "Epoch 81/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.9024 - mae: 220.7265 - val_loss: 200.0654 - val_mae: 199.8892\n",
            "Epoch 82/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 223.0801 - mae: 222.9040 - val_loss: 199.9053 - val_mae: 199.7294\n",
            "Epoch 83/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.2692 - mae: 220.0931 - val_loss: 199.9327 - val_mae: 199.7564\n",
            "Epoch 84/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.4011 - mae: 221.2248 - val_loss: 198.1461 - val_mae: 197.9696\n",
            "Epoch 85/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.8619 - mae: 220.6853 - val_loss: 198.6999 - val_mae: 198.5228\n",
            "Epoch 86/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.3286 - mae: 220.1515 - val_loss: 200.3306 - val_mae: 200.1535\n",
            "Epoch 87/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.2388 - mae: 217.0617 - val_loss: 199.8646 - val_mae: 199.6873\n",
            "Epoch 88/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.3741 - mae: 216.1964 - val_loss: 199.8637 - val_mae: 199.6858\n",
            "Epoch 89/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.4036 - mae: 218.2255 - val_loss: 198.9292 - val_mae: 198.7510\n",
            "Epoch 90/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.6683 - mae: 218.4899 - val_loss: 200.9682 - val_mae: 200.7896\n",
            "Epoch 91/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.0404 - mae: 215.8616 - val_loss: 199.3929 - val_mae: 199.2137\n",
            "Epoch 92/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.9339 - mae: 218.7543 - val_loss: 200.0345 - val_mae: 199.8550\n",
            "Epoch 93/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.3700 - mae: 219.1904 - val_loss: 197.9015 - val_mae: 197.7215\n",
            "Epoch 94/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.2143 - mae: 218.0344 - val_loss: 202.7397 - val_mae: 202.5597\n",
            "Epoch 95/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.3997 - mae: 217.2195 - val_loss: 201.1599 - val_mae: 200.9795\n",
            "Epoch 96/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.1630 - mae: 216.9823 - val_loss: 201.9796 - val_mae: 201.7988\n",
            "Epoch 97/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.3807 - mae: 217.2000 - val_loss: 199.2572 - val_mae: 199.0765\n",
            "Epoch 98/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.8677 - mae: 218.6868 - val_loss: 198.7809 - val_mae: 198.6000\n",
            "Epoch 99/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.5724 - mae: 219.3913 - val_loss: 199.9377 - val_mae: 199.7563\n",
            "Epoch 100/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.7881 - mae: 217.6068 - val_loss: 203.7270 - val_mae: 203.5455\n",
            "Epoch 101/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.2470 - mae: 217.0656 - val_loss: 204.4440 - val_mae: 204.2628\n",
            "Epoch 102/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.6667 - mae: 219.4851 - val_loss: 201.6012 - val_mae: 201.4194\n",
            "Epoch 103/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.6725 - mae: 219.4906 - val_loss: 202.1117 - val_mae: 201.9296\n",
            "Epoch 104/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.8683 - mae: 219.6859 - val_loss: 204.1965 - val_mae: 204.0139\n",
            "Epoch 105/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.1538 - mae: 217.9711 - val_loss: 197.9586 - val_mae: 197.7759\n",
            "Epoch 106/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.1231 - mae: 216.9402 - val_loss: 201.7299 - val_mae: 201.5471\n",
            "Epoch 107/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.6019 - mae: 218.4192 - val_loss: 200.4723 - val_mae: 200.2894\n",
            "Epoch 108/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.7134 - mae: 219.5305 - val_loss: 201.2909 - val_mae: 201.1080\n",
            "Epoch 109/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.5211 - mae: 216.3383 - val_loss: 201.3142 - val_mae: 201.1310\n",
            "Epoch 110/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.7575 - mae: 216.5742 - val_loss: 200.7810 - val_mae: 200.5975\n",
            "Epoch 111/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.9048 - mae: 213.7211 - val_loss: 198.8308 - val_mae: 198.6467\n",
            "Epoch 112/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.7760 - mae: 217.5918 - val_loss: 199.6402 - val_mae: 199.4561\n",
            "Epoch 113/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.1245 - mae: 217.9401 - val_loss: 202.4048 - val_mae: 202.2207\n",
            "Epoch 114/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.9220 - mae: 217.7375 - val_loss: 198.9262 - val_mae: 198.7413\n",
            "Epoch 115/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.3929 - mae: 214.2080 - val_loss: 198.4760 - val_mae: 198.2910\n",
            "Epoch 116/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.5976 - mae: 214.4126 - val_loss: 198.9368 - val_mae: 198.7518\n",
            "Epoch 117/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.6329 - mae: 214.4477 - val_loss: 197.9308 - val_mae: 197.7453\n",
            "Epoch 118/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.6306 - mae: 216.4450 - val_loss: 199.4297 - val_mae: 199.2440\n",
            "Epoch 119/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.0582 - mae: 215.8725 - val_loss: 200.2506 - val_mae: 200.0648\n",
            "Epoch 120/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.2481 - mae: 214.0621 - val_loss: 198.6503 - val_mae: 198.4642\n",
            "Epoch 121/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.2185 - mae: 218.0320 - val_loss: 198.6793 - val_mae: 198.4925\n",
            "Epoch 122/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.5863 - mae: 219.3996 - val_loss: 200.3026 - val_mae: 200.1157\n",
            "Epoch 123/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.2755 - mae: 217.0883 - val_loss: 201.2731 - val_mae: 201.0859\n",
            "Epoch 124/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.1375 - mae: 217.9504 - val_loss: 197.2841 - val_mae: 197.0966\n",
            "Epoch 125/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 215.7829 - mae: 215.5952 - val_loss: 200.7899 - val_mae: 200.6018\n",
            "Epoch 126/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.6391 - mae: 214.4511 - val_loss: 198.7648 - val_mae: 198.5771\n",
            "Epoch 127/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 215.1266 - mae: 214.9386 - val_loss: 198.1499 - val_mae: 197.9618\n",
            "Epoch 128/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.8738 - mae: 214.6855 - val_loss: 199.9553 - val_mae: 199.7671\n",
            "Epoch 129/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.6476 - mae: 217.4593 - val_loss: 198.5090 - val_mae: 198.3207\n",
            "Epoch 130/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.9252 - mae: 217.7370 - val_loss: 199.1353 - val_mae: 198.9470\n",
            "Epoch 131/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.3851 - mae: 216.1965 - val_loss: 199.2725 - val_mae: 199.0835\n",
            "Epoch 132/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.8256 - mae: 217.6367 - val_loss: 198.7617 - val_mae: 198.5728\n",
            "Epoch 133/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 213.8823 - mae: 213.6931 - val_loss: 200.3371 - val_mae: 200.1476\n",
            "Epoch 134/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.3670 - mae: 216.1773 - val_loss: 198.8780 - val_mae: 198.6881\n",
            "Epoch 135/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.2319 - mae: 219.0418 - val_loss: 198.7472 - val_mae: 198.5570\n",
            "Epoch 136/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.9005 - mae: 214.7099 - val_loss: 199.8771 - val_mae: 199.6865\n",
            "Epoch 137/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4708 - mae: 214.2800 - val_loss: 198.6460 - val_mae: 198.4550\n",
            "Epoch 138/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 213.7861 - mae: 213.5950 - val_loss: 201.3168 - val_mae: 201.1255\n",
            "Epoch 139/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1152 - mae: 213.9237 - val_loss: 198.8316 - val_mae: 198.6400\n",
            "Epoch 140/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.8564 - mae: 216.6647 - val_loss: 198.6011 - val_mae: 198.4093\n",
            "Epoch 141/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.8209 - mae: 218.6290 - val_loss: 200.3371 - val_mae: 200.1450\n",
            "Epoch 142/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.6143 - mae: 216.4222 - val_loss: 200.0134 - val_mae: 199.8210\n",
            "Epoch 143/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.3215 - mae: 218.1288 - val_loss: 200.7939 - val_mae: 200.6009\n",
            "Epoch 144/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 215.9725 - mae: 215.7794 - val_loss: 198.0704 - val_mae: 197.8770\n",
            "Epoch 145/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 213.1392 - mae: 212.9459 - val_loss: 201.4857 - val_mae: 201.2924\n",
            "Epoch 146/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.8588 - mae: 214.6654 - val_loss: 201.8507 - val_mae: 201.6572\n",
            "Epoch 147/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 213.4575 - mae: 213.2640 - val_loss: 201.3627 - val_mae: 201.1690\n",
            "Epoch 148/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.1751 - mae: 216.9812 - val_loss: 200.4000 - val_mae: 200.2058\n",
            "Epoch 149/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 213.4780 - mae: 213.2837 - val_loss: 201.2564 - val_mae: 201.0621\n",
            "Epoch 150/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 215.9551 - mae: 215.7606 - val_loss: 201.5505 - val_mae: 201.3559\n",
            "Processing fold #: 1\n",
            "Validation data includes observations 1421 through 2841\n",
            "Training data includes observations 0 through 1420 joined with observations 2842 through the final observation.\n",
            "\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - 1s 8ms/step - loss: 756.3830 - mae: 756.2097 - val_loss: 760.1631 - val_mae: 759.9985\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 753.8613 - mae: 753.7056 - val_loss: 748.6709 - val_mae: 748.5248\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 748.7074 - mae: 748.5698 - val_loss: 745.4185 - val_mae: 745.2895\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 734.4203 - mae: 734.2979 - val_loss: 737.8127 - val_mae: 737.6964\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 698.9818 - mae: 698.8690 - val_loss: 731.7979 - val_mae: 731.6882\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 627.3202 - mae: 627.2117 - val_loss: 661.3611 - val_mae: 661.2531\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 521.6201 - mae: 521.5114 - val_loss: 575.6113 - val_mae: 575.5018\n",
            "Epoch 8/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 431.6635 - mae: 431.5529 - val_loss: 532.5756 - val_mae: 532.4639\n",
            "Epoch 9/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 395.1123 - mae: 394.9996 - val_loss: 477.6177 - val_mae: 477.5041\n",
            "Epoch 10/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 383.0742 - mae: 382.9596 - val_loss: 484.4094 - val_mae: 484.2939\n",
            "Epoch 11/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 380.7408 - mae: 380.6233 - val_loss: 495.5803 - val_mae: 495.4608\n",
            "Epoch 12/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 370.0189 - mae: 369.8978 - val_loss: 491.6530 - val_mae: 491.5305\n",
            "Epoch 13/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 356.7936 - mae: 356.6698 - val_loss: 489.1956 - val_mae: 489.0704\n",
            "Epoch 14/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 334.2125 - mae: 334.0862 - val_loss: 500.1341 - val_mae: 500.0063\n",
            "Epoch 15/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 301.9735 - mae: 301.8441 - val_loss: 491.0923 - val_mae: 490.9617\n",
            "Epoch 16/150\n",
            "45/45 [==============================] - 0s 5ms/step - loss: 275.9959 - mae: 275.8648 - val_loss: 409.1588 - val_mae: 409.0271\n",
            "Epoch 17/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 265.0945 - mae: 264.9622 - val_loss: 360.2103 - val_mae: 360.0772\n",
            "Epoch 18/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 261.7416 - mae: 261.6081 - val_loss: 306.9001 - val_mae: 306.7662\n",
            "Epoch 19/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 259.4409 - mae: 259.3062 - val_loss: 260.5594 - val_mae: 260.4241\n",
            "Epoch 20/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 251.7988 - mae: 251.6638 - val_loss: 234.8828 - val_mae: 234.7478\n",
            "Epoch 21/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 255.5266 - mae: 255.3915 - val_loss: 238.5790 - val_mae: 238.4435\n",
            "Epoch 22/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 250.8908 - mae: 250.7547 - val_loss: 227.4895 - val_mae: 227.3531\n",
            "Epoch 23/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 246.3857 - mae: 246.2492 - val_loss: 217.5856 - val_mae: 217.4490\n",
            "Epoch 24/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 247.5723 - mae: 247.4355 - val_loss: 217.7025 - val_mae: 217.5656\n",
            "Epoch 25/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 245.3753 - mae: 245.2383 - val_loss: 217.2213 - val_mae: 217.0845\n",
            "Epoch 26/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.4664 - mae: 243.3295 - val_loss: 215.6302 - val_mae: 215.4932\n",
            "Epoch 27/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 245.4843 - mae: 245.3475 - val_loss: 214.8073 - val_mae: 214.6701\n",
            "Epoch 28/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 242.6504 - mae: 242.5132 - val_loss: 215.0313 - val_mae: 214.8941\n",
            "Epoch 29/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.0540 - mae: 242.9167 - val_loss: 214.5033 - val_mae: 214.3659\n",
            "Epoch 30/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 239.2500 - mae: 239.1121 - val_loss: 213.5854 - val_mae: 213.4477\n",
            "Epoch 31/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 240.1836 - mae: 240.0457 - val_loss: 213.4665 - val_mae: 213.3285\n",
            "Epoch 32/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.5143 - mae: 231.3760 - val_loss: 211.3443 - val_mae: 211.2059\n",
            "Epoch 33/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 238.3933 - mae: 238.2551 - val_loss: 209.6793 - val_mae: 209.5410\n",
            "Epoch 34/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 236.3208 - mae: 236.1825 - val_loss: 212.5000 - val_mae: 212.3615\n",
            "Epoch 35/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 233.0160 - mae: 232.8774 - val_loss: 208.9707 - val_mae: 208.8322\n",
            "Epoch 36/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 232.4184 - mae: 232.2799 - val_loss: 212.0416 - val_mae: 211.9030\n",
            "Epoch 37/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 234.6067 - mae: 234.4681 - val_loss: 209.1005 - val_mae: 208.9621\n",
            "Epoch 38/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 234.3227 - mae: 234.1840 - val_loss: 209.4829 - val_mae: 209.3442\n",
            "Epoch 39/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 235.4229 - mae: 235.2842 - val_loss: 207.9813 - val_mae: 207.8425\n",
            "Epoch 40/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.2126 - mae: 232.0739 - val_loss: 208.8298 - val_mae: 208.6909\n",
            "Epoch 41/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.6942 - mae: 228.5553 - val_loss: 207.8722 - val_mae: 207.7334\n",
            "Epoch 42/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 235.6850 - mae: 235.5463 - val_loss: 211.5906 - val_mae: 211.4520\n",
            "Epoch 43/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 228.0940 - mae: 227.9552 - val_loss: 208.6575 - val_mae: 208.5187\n",
            "Epoch 44/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 230.8875 - mae: 230.7485 - val_loss: 206.6493 - val_mae: 206.5103\n",
            "Epoch 45/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.3238 - mae: 231.1846 - val_loss: 210.9873 - val_mae: 210.8479\n",
            "Epoch 46/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 233.7173 - mae: 233.5778 - val_loss: 210.3792 - val_mae: 210.2394\n",
            "Epoch 47/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.0795 - mae: 230.9396 - val_loss: 209.1402 - val_mae: 209.0005\n",
            "Epoch 48/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.4549 - mae: 231.3152 - val_loss: 208.4113 - val_mae: 208.2715\n",
            "Epoch 49/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 230.3432 - mae: 230.2032 - val_loss: 209.8999 - val_mae: 209.7600\n",
            "Epoch 50/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 228.4148 - mae: 228.2750 - val_loss: 212.0190 - val_mae: 211.8791\n",
            "Epoch 51/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 229.6706 - mae: 229.5304 - val_loss: 206.5008 - val_mae: 206.3606\n",
            "Epoch 52/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.1868 - mae: 230.0465 - val_loss: 205.6694 - val_mae: 205.5289\n",
            "Epoch 53/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 227.9077 - mae: 227.7672 - val_loss: 205.9204 - val_mae: 205.7800\n",
            "Epoch 54/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.7721 - mae: 230.6316 - val_loss: 207.3683 - val_mae: 207.2277\n",
            "Epoch 55/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 228.8560 - mae: 228.7153 - val_loss: 208.8386 - val_mae: 208.6980\n",
            "Epoch 56/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 227.7822 - mae: 227.6415 - val_loss: 206.9941 - val_mae: 206.8533\n",
            "Epoch 57/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.8878 - mae: 229.7469 - val_loss: 208.2009 - val_mae: 208.0598\n",
            "Epoch 58/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 227.2664 - mae: 227.1254 - val_loss: 206.8798 - val_mae: 206.7388\n",
            "Epoch 59/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.1767 - mae: 225.0355 - val_loss: 206.0326 - val_mae: 205.8912\n",
            "Epoch 60/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.3260 - mae: 227.1842 - val_loss: 208.4277 - val_mae: 208.2857\n",
            "Epoch 61/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.4548 - mae: 224.3128 - val_loss: 208.6104 - val_mae: 208.4684\n",
            "Epoch 62/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.5647 - mae: 224.4227 - val_loss: 208.4702 - val_mae: 208.3282\n",
            "Epoch 63/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.4310 - mae: 224.2888 - val_loss: 206.1098 - val_mae: 205.9676\n",
            "Epoch 64/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.1420 - mae: 223.9995 - val_loss: 207.5997 - val_mae: 207.4572\n",
            "Epoch 65/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.2420 - mae: 224.0993 - val_loss: 206.1043 - val_mae: 205.9616\n",
            "Epoch 66/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.5628 - mae: 225.4198 - val_loss: 207.5675 - val_mae: 207.4243\n",
            "Epoch 67/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.5849 - mae: 225.4415 - val_loss: 208.2635 - val_mae: 208.1198\n",
            "Epoch 68/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.4700 - mae: 224.3263 - val_loss: 206.8522 - val_mae: 206.7085\n",
            "Epoch 69/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.8455 - mae: 225.7019 - val_loss: 206.8913 - val_mae: 206.7475\n",
            "Epoch 70/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.7374 - mae: 224.5934 - val_loss: 206.0930 - val_mae: 205.9489\n",
            "Epoch 71/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.8652 - mae: 222.7210 - val_loss: 205.5831 - val_mae: 205.4390\n",
            "Epoch 72/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.9723 - mae: 225.8280 - val_loss: 205.4820 - val_mae: 205.3374\n",
            "Epoch 73/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.1021 - mae: 219.9575 - val_loss: 205.5209 - val_mae: 205.3761\n",
            "Epoch 74/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.9514 - mae: 220.8063 - val_loss: 204.3472 - val_mae: 204.2018\n",
            "Epoch 75/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.2734 - mae: 221.1277 - val_loss: 205.4313 - val_mae: 205.2857\n",
            "Epoch 76/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.7376 - mae: 222.5920 - val_loss: 204.9947 - val_mae: 204.8490\n",
            "Epoch 77/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.0132 - mae: 225.8676 - val_loss: 207.5482 - val_mae: 207.4025\n",
            "Epoch 78/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.4944 - mae: 220.3483 - val_loss: 205.9796 - val_mae: 205.8336\n",
            "Epoch 79/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.7678 - mae: 222.6218 - val_loss: 206.3735 - val_mae: 206.2276\n",
            "Epoch 80/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.9861 - mae: 222.8398 - val_loss: 205.0434 - val_mae: 204.8968\n",
            "Epoch 81/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 226.1727 - mae: 226.0260 - val_loss: 206.2127 - val_mae: 206.0658\n",
            "Epoch 82/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.7877 - mae: 223.6410 - val_loss: 204.8149 - val_mae: 204.6682\n",
            "Epoch 83/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.6821 - mae: 224.5353 - val_loss: 206.9907 - val_mae: 206.8439\n",
            "Epoch 84/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.7451 - mae: 222.5982 - val_loss: 207.7587 - val_mae: 207.6117\n",
            "Epoch 85/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.2743 - mae: 223.1272 - val_loss: 206.3613 - val_mae: 206.2139\n",
            "Epoch 86/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 223.1840 - mae: 223.0366 - val_loss: 204.4088 - val_mae: 204.2612\n",
            "Epoch 87/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 226.1720 - mae: 226.0243 - val_loss: 206.3870 - val_mae: 206.2391\n",
            "Epoch 88/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.8682 - mae: 220.7202 - val_loss: 207.4424 - val_mae: 207.2946\n",
            "Epoch 89/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.0610 - mae: 219.9128 - val_loss: 205.3097 - val_mae: 205.1615\n",
            "Epoch 90/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.9670 - mae: 222.8188 - val_loss: 203.9070 - val_mae: 203.7588\n",
            "Epoch 91/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.5583 - mae: 219.4099 - val_loss: 203.9323 - val_mae: 203.7837\n",
            "Epoch 92/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.8848 - mae: 224.7362 - val_loss: 206.5254 - val_mae: 206.3769\n",
            "Epoch 93/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 222.3097 - mae: 222.1608 - val_loss: 204.9509 - val_mae: 204.8018\n",
            "Epoch 94/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 225.0750 - mae: 224.9257 - val_loss: 205.0913 - val_mae: 204.9418\n",
            "Epoch 95/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.2898 - mae: 220.1398 - val_loss: 204.1422 - val_mae: 203.9921\n",
            "Epoch 96/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.1320 - mae: 221.9816 - val_loss: 203.4075 - val_mae: 203.2569\n",
            "Epoch 97/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.7900 - mae: 219.6395 - val_loss: 204.4803 - val_mae: 204.3297\n",
            "Epoch 98/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.9918 - mae: 220.8411 - val_loss: 205.2213 - val_mae: 205.0708\n",
            "Epoch 99/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.0316 - mae: 217.8809 - val_loss: 205.3615 - val_mae: 205.2105\n",
            "Epoch 100/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.5443 - mae: 220.3931 - val_loss: 205.3542 - val_mae: 205.2028\n",
            "Epoch 101/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.8994 - mae: 224.7481 - val_loss: 206.6970 - val_mae: 206.5457\n",
            "Epoch 102/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.2733 - mae: 221.1221 - val_loss: 205.3406 - val_mae: 205.1892\n",
            "Epoch 103/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.1728 - mae: 218.0213 - val_loss: 204.2154 - val_mae: 204.0639\n",
            "Epoch 104/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.4308 - mae: 220.2791 - val_loss: 204.5802 - val_mae: 204.4283\n",
            "Epoch 105/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.2252 - mae: 220.0731 - val_loss: 204.8163 - val_mae: 204.6639\n",
            "Epoch 106/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 219.6657 - mae: 219.5132 - val_loss: 204.3587 - val_mae: 204.2062\n",
            "Epoch 107/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.5692 - mae: 216.4165 - val_loss: 204.3025 - val_mae: 204.1497\n",
            "Epoch 108/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.4373 - mae: 217.2845 - val_loss: 207.0963 - val_mae: 206.9435\n",
            "Epoch 109/150\n",
            "45/45 [==============================] - 0s 5ms/step - loss: 219.6639 - mae: 219.5107 - val_loss: 204.1062 - val_mae: 203.9525\n",
            "Epoch 110/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 220.0594 - mae: 219.9058 - val_loss: 206.0899 - val_mae: 205.9360\n",
            "Epoch 111/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.2501 - mae: 219.0961 - val_loss: 203.2099 - val_mae: 203.0558\n",
            "Epoch 112/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.8767 - mae: 216.7226 - val_loss: 204.2814 - val_mae: 204.1272\n",
            "Epoch 113/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.8721 - mae: 219.7179 - val_loss: 204.1519 - val_mae: 203.9978\n",
            "Epoch 114/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.5205 - mae: 220.3663 - val_loss: 204.2119 - val_mae: 204.0578\n",
            "Epoch 115/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.6416 - mae: 217.4875 - val_loss: 203.6075 - val_mae: 203.4532\n",
            "Epoch 116/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.1350 - mae: 217.9806 - val_loss: 205.2092 - val_mae: 205.0548\n",
            "Epoch 117/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.4561 - mae: 217.3015 - val_loss: 205.3749 - val_mae: 205.2202\n",
            "Epoch 118/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.2617 - mae: 219.1069 - val_loss: 205.4518 - val_mae: 205.2968\n",
            "Epoch 119/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.9819 - mae: 217.8271 - val_loss: 205.4998 - val_mae: 205.3451\n",
            "Epoch 120/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.9569 - mae: 218.8018 - val_loss: 204.8549 - val_mae: 204.6996\n",
            "Epoch 121/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.6903 - mae: 217.5347 - val_loss: 204.7375 - val_mae: 204.5819\n",
            "Epoch 122/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.7595 - mae: 218.6037 - val_loss: 203.5972 - val_mae: 203.4412\n",
            "Epoch 123/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.8808 - mae: 217.7249 - val_loss: 205.5334 - val_mae: 205.3772\n",
            "Epoch 124/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.8670 - mae: 215.7108 - val_loss: 202.0906 - val_mae: 201.9346\n",
            "Epoch 125/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.3839 - mae: 219.2275 - val_loss: 203.7951 - val_mae: 203.6386\n",
            "Epoch 126/150\n",
            "45/45 [==============================] - 0s 9ms/step - loss: 216.4673 - mae: 216.3105 - val_loss: 207.0276 - val_mae: 206.8709\n",
            "Epoch 127/150\n",
            "45/45 [==============================] - 0s 8ms/step - loss: 219.7383 - mae: 219.5812 - val_loss: 204.4725 - val_mae: 204.3152\n",
            "Epoch 128/150\n",
            "45/45 [==============================] - 0s 5ms/step - loss: 215.2287 - mae: 215.0712 - val_loss: 205.1233 - val_mae: 204.9657\n",
            "Epoch 129/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.2932 - mae: 215.1353 - val_loss: 205.0842 - val_mae: 204.9264\n",
            "Epoch 130/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.0242 - mae: 215.8662 - val_loss: 206.4411 - val_mae: 206.2832\n",
            "Epoch 131/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.5272 - mae: 217.3690 - val_loss: 205.0321 - val_mae: 204.8739\n",
            "Epoch 132/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.7413 - mae: 217.5831 - val_loss: 203.8919 - val_mae: 203.7337\n",
            "Epoch 133/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.9864 - mae: 217.8279 - val_loss: 205.5622 - val_mae: 205.4036\n",
            "Epoch 134/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.1201 - mae: 216.9614 - val_loss: 206.1273 - val_mae: 205.9684\n",
            "Epoch 135/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 218.9651 - mae: 218.8063 - val_loss: 205.7062 - val_mae: 205.5474\n",
            "Epoch 136/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.9283 - mae: 214.7697 - val_loss: 204.4438 - val_mae: 204.2852\n",
            "Epoch 137/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.7118 - mae: 217.5530 - val_loss: 205.8022 - val_mae: 205.6434\n",
            "Epoch 138/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.2154 - mae: 216.0563 - val_loss: 204.4012 - val_mae: 204.2420\n",
            "Epoch 139/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.6703 - mae: 214.5107 - val_loss: 205.7374 - val_mae: 205.5775\n",
            "Epoch 140/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.1011 - mae: 217.9409 - val_loss: 203.7794 - val_mae: 203.6190\n",
            "Epoch 141/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 215.0455 - mae: 214.8850 - val_loss: 206.3172 - val_mae: 206.1564\n",
            "Epoch 142/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.7043 - mae: 215.5434 - val_loss: 207.2758 - val_mae: 207.1147\n",
            "Epoch 143/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.8272 - mae: 216.6662 - val_loss: 204.6906 - val_mae: 204.5295\n",
            "Epoch 144/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 216.8096 - mae: 216.6484 - val_loss: 206.0002 - val_mae: 205.8391\n",
            "Epoch 145/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.3488 - mae: 216.1878 - val_loss: 204.7577 - val_mae: 204.5965\n",
            "Epoch 146/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.7000 - mae: 214.5388 - val_loss: 205.5108 - val_mae: 205.3493\n",
            "Epoch 147/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.9891 - mae: 213.8277 - val_loss: 207.8354 - val_mae: 207.6738\n",
            "Epoch 148/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.8039 - mae: 214.6424 - val_loss: 204.4340 - val_mae: 204.2723\n",
            "Epoch 149/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1409 - mae: 213.9790 - val_loss: 205.1597 - val_mae: 204.9978\n",
            "Epoch 150/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.8833 - mae: 213.7212 - val_loss: 205.8817 - val_mae: 205.7192\n",
            "Processing fold #: 2\n",
            "Validation data includes observations 2842 through 4262\n",
            "Training data includes observations 0 through 2841 joined with observations 4263 through the final observation.\n",
            "\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - 1s 8ms/step - loss: 754.8384 - mae: 754.6578 - val_loss: 744.0347 - val_mae: 743.8596\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 749.9081 - mae: 749.7398 - val_loss: 735.8392 - val_mae: 735.6783\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 740.4107 - mae: 740.2565 - val_loss: 744.1750 - val_mae: 744.0278\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 719.1845 - mae: 719.0418 - val_loss: 743.6330 - val_mae: 743.4946\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 675.4617 - mae: 675.3266 - val_loss: 720.0225 - val_mae: 719.8907\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 598.7618 - mae: 598.6321 - val_loss: 623.6730 - val_mae: 623.5454\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 503.3749 - mae: 503.2476 - val_loss: 540.4380 - val_mae: 540.3110\n",
            "Epoch 8/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 430.8612 - mae: 430.7336 - val_loss: 551.6216 - val_mae: 551.4935\n",
            "Epoch 9/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 395.1176 - mae: 394.9891 - val_loss: 523.4819 - val_mae: 523.3525\n",
            "Epoch 10/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 380.6001 - mae: 380.4694 - val_loss: 615.0302 - val_mae: 614.8981\n",
            "Epoch 11/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 363.9584 - mae: 363.8252 - val_loss: 593.0560 - val_mae: 592.9214\n",
            "Epoch 12/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 346.8250 - mae: 346.6888 - val_loss: 587.9148 - val_mae: 587.7770\n",
            "Epoch 13/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 328.6654 - mae: 328.5266 - val_loss: 573.6722 - val_mae: 573.5323\n",
            "Epoch 14/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 318.7336 - mae: 318.5929 - val_loss: 499.0360 - val_mae: 498.8943\n",
            "Epoch 15/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 309.1144 - mae: 308.9720 - val_loss: 423.0924 - val_mae: 422.9489\n",
            "Epoch 16/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 296.4464 - mae: 296.3020 - val_loss: 371.8286 - val_mae: 371.6830\n",
            "Epoch 17/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 277.8323 - mae: 277.6861 - val_loss: 315.2262 - val_mae: 315.0798\n",
            "Epoch 18/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 275.9882 - mae: 275.8418 - val_loss: 288.6328 - val_mae: 288.4859\n",
            "Epoch 19/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 268.1636 - mae: 268.0162 - val_loss: 269.9746 - val_mae: 269.8267\n",
            "Epoch 20/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 262.2952 - mae: 262.1469 - val_loss: 259.8838 - val_mae: 259.7350\n",
            "Epoch 21/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 256.2974 - mae: 256.1482 - val_loss: 252.6335 - val_mae: 252.4841\n",
            "Epoch 22/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 257.8378 - mae: 257.6881 - val_loss: 245.8415 - val_mae: 245.6914\n",
            "Epoch 23/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 249.8091 - mae: 249.6584 - val_loss: 243.7203 - val_mae: 243.5692\n",
            "Epoch 24/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 253.1777 - mae: 253.0263 - val_loss: 234.9704 - val_mae: 234.8186\n",
            "Epoch 25/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 247.0454 - mae: 246.8933 - val_loss: 238.5619 - val_mae: 238.4095\n",
            "Epoch 26/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.4657 - mae: 243.3131 - val_loss: 228.3990 - val_mae: 228.2460\n",
            "Epoch 27/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 238.1132 - mae: 237.9599 - val_loss: 225.6043 - val_mae: 225.4506\n",
            "Epoch 28/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 235.7781 - mae: 235.6241 - val_loss: 222.5847 - val_mae: 222.4303\n",
            "Epoch 29/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 242.0974 - mae: 241.9426 - val_loss: 222.0163 - val_mae: 221.8612\n",
            "Epoch 30/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.1573 - mae: 232.0020 - val_loss: 220.2479 - val_mae: 220.0923\n",
            "Epoch 31/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 234.0191 - mae: 233.8633 - val_loss: 220.2115 - val_mae: 220.0555\n",
            "Epoch 32/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 237.5382 - mae: 237.3820 - val_loss: 219.1397 - val_mae: 218.9834\n",
            "Epoch 33/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.3813 - mae: 232.2247 - val_loss: 218.4165 - val_mae: 218.2594\n",
            "Epoch 34/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.9411 - mae: 229.7837 - val_loss: 217.1793 - val_mae: 217.0217\n",
            "Epoch 35/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.7807 - mae: 231.6230 - val_loss: 219.3900 - val_mae: 219.2322\n",
            "Epoch 36/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.9006 - mae: 231.7425 - val_loss: 218.4537 - val_mae: 218.2954\n",
            "Epoch 37/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.6816 - mae: 230.5230 - val_loss: 220.5200 - val_mae: 220.3613\n",
            "Epoch 38/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.3866 - mae: 231.2277 - val_loss: 219.0342 - val_mae: 218.8751\n",
            "Epoch 39/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 228.4140 - mae: 228.2547 - val_loss: 219.6444 - val_mae: 219.4850\n",
            "Epoch 40/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.5639 - mae: 228.4042 - val_loss: 219.8634 - val_mae: 219.7036\n",
            "Epoch 41/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.8231 - mae: 228.6628 - val_loss: 219.2752 - val_mae: 219.1146\n",
            "Epoch 42/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.9543 - mae: 226.7936 - val_loss: 216.4766 - val_mae: 216.3156\n",
            "Epoch 43/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.5070 - mae: 229.3457 - val_loss: 216.0540 - val_mae: 215.8926\n",
            "Epoch 44/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.7313 - mae: 228.5697 - val_loss: 217.7702 - val_mae: 217.6085\n",
            "Epoch 45/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.0244 - mae: 225.8625 - val_loss: 217.1683 - val_mae: 217.0062\n",
            "Epoch 46/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.7417 - mae: 226.5793 - val_loss: 215.1933 - val_mae: 215.0307\n",
            "Epoch 47/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.3760 - mae: 225.2134 - val_loss: 214.1904 - val_mae: 214.0275\n",
            "Epoch 48/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.8324 - mae: 227.6695 - val_loss: 215.2060 - val_mae: 215.0429\n",
            "Epoch 49/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.9549 - mae: 223.7915 - val_loss: 214.6518 - val_mae: 214.4882\n",
            "Epoch 50/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.6906 - mae: 225.5268 - val_loss: 214.8481 - val_mae: 214.6839\n",
            "Epoch 51/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.6360 - mae: 225.4717 - val_loss: 214.5544 - val_mae: 214.3899\n",
            "Epoch 52/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.9016 - mae: 225.7369 - val_loss: 214.4168 - val_mae: 214.2517\n",
            "Epoch 53/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.3814 - mae: 226.2163 - val_loss: 216.3767 - val_mae: 216.2112\n",
            "Epoch 54/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.2311 - mae: 228.0656 - val_loss: 215.2808 - val_mae: 215.1151\n",
            "Epoch 55/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.6277 - mae: 224.4619 - val_loss: 215.8313 - val_mae: 215.6653\n",
            "Epoch 56/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.1340 - mae: 222.9677 - val_loss: 215.6270 - val_mae: 215.4604\n",
            "Epoch 57/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 224.4166 - mae: 224.2498 - val_loss: 213.0814 - val_mae: 212.9144\n",
            "Epoch 58/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.6759 - mae: 223.5087 - val_loss: 215.6010 - val_mae: 215.4335\n",
            "Epoch 59/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.2902 - mae: 223.1225 - val_loss: 213.4043 - val_mae: 213.2363\n",
            "Epoch 60/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.5448 - mae: 220.3769 - val_loss: 214.2373 - val_mae: 214.0693\n",
            "Epoch 61/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.0021 - mae: 221.8340 - val_loss: 214.5877 - val_mae: 214.4195\n",
            "Epoch 62/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.3603 - mae: 225.1919 - val_loss: 222.6786 - val_mae: 222.5101\n",
            "Epoch 63/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.5775 - mae: 223.4090 - val_loss: 217.4804 - val_mae: 217.3118\n",
            "Epoch 64/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.6026 - mae: 220.4338 - val_loss: 220.0806 - val_mae: 219.9116\n",
            "Epoch 65/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.4886 - mae: 217.3191 - val_loss: 212.3501 - val_mae: 212.1802\n",
            "Epoch 66/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.5272 - mae: 222.3571 - val_loss: 214.8730 - val_mae: 214.7028\n",
            "Epoch 67/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.5989 - mae: 223.4286 - val_loss: 212.9179 - val_mae: 212.7477\n",
            "Epoch 68/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.8945 - mae: 221.7240 - val_loss: 214.6292 - val_mae: 214.4585\n",
            "Epoch 69/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.4245 - mae: 219.2535 - val_loss: 211.4907 - val_mae: 211.3195\n",
            "Epoch 70/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.3670 - mae: 218.1956 - val_loss: 212.9476 - val_mae: 212.7760\n",
            "Epoch 71/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.1828 - mae: 221.0107 - val_loss: 212.5673 - val_mae: 212.3949\n",
            "Epoch 72/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.8120 - mae: 219.6393 - val_loss: 212.8829 - val_mae: 212.7100\n",
            "Epoch 73/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.2643 - mae: 224.0914 - val_loss: 211.6016 - val_mae: 211.4285\n",
            "Epoch 74/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.9389 - mae: 217.7654 - val_loss: 214.0723 - val_mae: 213.8985\n",
            "Epoch 75/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.7585 - mae: 217.5845 - val_loss: 211.8121 - val_mae: 211.6382\n",
            "Epoch 76/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.7264 - mae: 219.5522 - val_loss: 210.7616 - val_mae: 210.5871\n",
            "Epoch 77/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 221.4453 - mae: 221.2705 - val_loss: 212.4568 - val_mae: 212.2819\n",
            "Epoch 78/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.3568 - mae: 217.1818 - val_loss: 212.5209 - val_mae: 212.3455\n",
            "Epoch 79/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.6367 - mae: 218.4612 - val_loss: 211.2655 - val_mae: 211.0898\n",
            "Epoch 80/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.0470 - mae: 215.8711 - val_loss: 211.4252 - val_mae: 211.2491\n",
            "Epoch 81/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.4241 - mae: 216.2477 - val_loss: 210.2249 - val_mae: 210.0480\n",
            "Epoch 82/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.3025 - mae: 217.1256 - val_loss: 211.6463 - val_mae: 211.4692\n",
            "Epoch 83/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.7828 - mae: 220.6055 - val_loss: 213.5481 - val_mae: 213.3706\n",
            "Epoch 84/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.0880 - mae: 218.9103 - val_loss: 210.1747 - val_mae: 209.9969\n",
            "Epoch 85/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.5190 - mae: 219.3409 - val_loss: 210.5844 - val_mae: 210.4061\n",
            "Epoch 86/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.2360 - mae: 216.0574 - val_loss: 214.3900 - val_mae: 214.2113\n",
            "Epoch 87/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.3955 - mae: 219.2164 - val_loss: 213.0368 - val_mae: 212.8575\n",
            "Epoch 88/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.0320 - mae: 216.8522 - val_loss: 213.1343 - val_mae: 212.9542\n",
            "Epoch 89/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.1620 - mae: 216.9816 - val_loss: 210.7253 - val_mae: 210.5445\n",
            "Epoch 90/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.7969 - mae: 213.6158 - val_loss: 211.5923 - val_mae: 211.4109\n",
            "Epoch 91/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.0817 - mae: 213.9002 - val_loss: 211.2129 - val_mae: 211.0311\n",
            "Epoch 92/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1543 - mae: 213.9724 - val_loss: 211.4590 - val_mae: 211.2769\n",
            "Epoch 93/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.7716 - mae: 214.5892 - val_loss: 210.3898 - val_mae: 210.2070\n",
            "Epoch 94/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.3452 - mae: 215.1622 - val_loss: 210.7891 - val_mae: 210.6059\n",
            "Epoch 95/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.6947 - mae: 215.5114 - val_loss: 212.3886 - val_mae: 212.2053\n",
            "Epoch 96/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.5179 - mae: 216.3344 - val_loss: 212.1319 - val_mae: 211.9482\n",
            "Epoch 97/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.2594 - mae: 217.0758 - val_loss: 211.5212 - val_mae: 211.3375\n",
            "Epoch 98/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4763 - mae: 214.2924 - val_loss: 211.9618 - val_mae: 211.7776\n",
            "Epoch 99/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 217.7234 - mae: 217.5390 - val_loss: 212.9060 - val_mae: 212.7214\n",
            "Epoch 100/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.8665 - mae: 214.6815 - val_loss: 211.8849 - val_mae: 211.6998\n",
            "Epoch 101/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.3704 - mae: 213.1849 - val_loss: 213.0607 - val_mae: 212.8751\n",
            "Epoch 102/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.0180 - mae: 211.8323 - val_loss: 212.0288 - val_mae: 211.8430\n",
            "Epoch 103/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.5325 - mae: 215.3466 - val_loss: 212.1958 - val_mae: 212.0096\n",
            "Epoch 104/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.0453 - mae: 213.8590 - val_loss: 211.8748 - val_mae: 211.6880\n",
            "Epoch 105/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.9763 - mae: 214.7896 - val_loss: 211.5840 - val_mae: 211.3970\n",
            "Epoch 106/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 214.0346 - mae: 213.8475 - val_loss: 211.8394 - val_mae: 211.6523\n",
            "Epoch 107/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.1774 - mae: 209.9900 - val_loss: 212.9145 - val_mae: 212.7267\n",
            "Epoch 108/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.2492 - mae: 215.0609 - val_loss: 212.6121 - val_mae: 212.4234\n",
            "Epoch 109/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.9457 - mae: 214.7568 - val_loss: 212.4010 - val_mae: 212.2120\n",
            "Epoch 110/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.1997 - mae: 215.0105 - val_loss: 211.3020 - val_mae: 211.1129\n",
            "Epoch 111/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.5712 - mae: 211.3821 - val_loss: 211.0715 - val_mae: 210.8824\n",
            "Epoch 112/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.8870 - mae: 212.6976 - val_loss: 212.0775 - val_mae: 211.8879\n",
            "Epoch 113/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.4671 - mae: 210.2775 - val_loss: 211.6497 - val_mae: 211.4596\n",
            "Epoch 114/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.4212 - mae: 213.2307 - val_loss: 211.6479 - val_mae: 211.4571\n",
            "Epoch 115/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.2153 - mae: 214.0240 - val_loss: 212.2509 - val_mae: 212.0592\n",
            "Epoch 116/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.0974 - mae: 210.9052 - val_loss: 210.6913 - val_mae: 210.4988\n",
            "Epoch 117/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.3930 - mae: 212.2001 - val_loss: 210.8807 - val_mae: 210.6877\n",
            "Epoch 118/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.1114 - mae: 210.9182 - val_loss: 210.7676 - val_mae: 210.5743\n",
            "Epoch 119/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.6069 - mae: 213.4132 - val_loss: 212.6504 - val_mae: 212.4566\n",
            "Epoch 120/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.5274 - mae: 211.3332 - val_loss: 212.0973 - val_mae: 211.9030\n",
            "Epoch 121/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1051 - mae: 213.9102 - val_loss: 212.9689 - val_mae: 212.7736\n",
            "Epoch 122/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.5435 - mae: 210.3481 - val_loss: 212.8607 - val_mae: 212.6652\n",
            "Epoch 123/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4970 - mae: 214.3009 - val_loss: 213.2994 - val_mae: 213.1032\n",
            "Epoch 124/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.6508 - mae: 210.4543 - val_loss: 212.6046 - val_mae: 212.4079\n",
            "Epoch 125/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.9503 - mae: 208.7536 - val_loss: 212.5065 - val_mae: 212.3098\n",
            "Epoch 126/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.1533 - mae: 210.9561 - val_loss: 213.0080 - val_mae: 212.8107\n",
            "Epoch 127/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.1493 - mae: 208.9516 - val_loss: 213.8085 - val_mae: 213.6105\n",
            "Epoch 128/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.8732 - mae: 210.6753 - val_loss: 213.4492 - val_mae: 213.2511\n",
            "Epoch 129/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.0685 - mae: 212.8703 - val_loss: 215.6969 - val_mae: 215.4987\n",
            "Epoch 130/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.0923 - mae: 209.8941 - val_loss: 213.5504 - val_mae: 213.3523\n",
            "Epoch 131/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 207.7624 - mae: 207.5640 - val_loss: 213.4944 - val_mae: 213.2955\n",
            "Epoch 132/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 207.1343 - mae: 206.9353 - val_loss: 212.9091 - val_mae: 212.7098\n",
            "Epoch 133/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.8123 - mae: 208.6129 - val_loss: 213.1247 - val_mae: 212.9250\n",
            "Epoch 134/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.5647 - mae: 209.3650 - val_loss: 212.4935 - val_mae: 212.2935\n",
            "Epoch 135/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.5509 - mae: 211.3506 - val_loss: 214.1774 - val_mae: 213.9768\n",
            "Epoch 136/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.3855 - mae: 211.1846 - val_loss: 215.0757 - val_mae: 214.8745\n",
            "Epoch 137/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.3801 - mae: 209.1788 - val_loss: 214.1274 - val_mae: 213.9261\n",
            "Epoch 138/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.6020 - mae: 209.4004 - val_loss: 213.4671 - val_mae: 213.2654\n",
            "Epoch 139/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.9677 - mae: 210.7658 - val_loss: 214.1336 - val_mae: 213.9317\n",
            "Epoch 140/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 208.1596 - mae: 207.9575 - val_loss: 213.1660 - val_mae: 212.9638\n",
            "Epoch 141/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.0154 - mae: 210.8129 - val_loss: 212.7695 - val_mae: 212.5668\n",
            "Epoch 142/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.1872 - mae: 209.9846 - val_loss: 211.3047 - val_mae: 211.1019\n",
            "Epoch 143/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 207.1954 - mae: 206.9924 - val_loss: 211.6344 - val_mae: 211.4312\n",
            "Epoch 144/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 206.1541 - mae: 205.9509 - val_loss: 213.7669 - val_mae: 213.5636\n",
            "Epoch 145/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.4420 - mae: 208.2385 - val_loss: 213.0699 - val_mae: 212.8663\n",
            "Epoch 146/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 209.3611 - mae: 209.1574 - val_loss: 213.6635 - val_mae: 213.4596\n",
            "Epoch 147/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 206.4037 - mae: 206.1995 - val_loss: 214.0139 - val_mae: 213.8096\n",
            "Epoch 148/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.8403 - mae: 209.6357 - val_loss: 213.7888 - val_mae: 213.5841\n",
            "Epoch 149/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.3664 - mae: 208.1613 - val_loss: 215.4947 - val_mae: 215.2893\n",
            "Epoch 150/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.5762 - mae: 208.3705 - val_loss: 212.4326 - val_mae: 212.2268\n",
            "Processing fold #: 3\n",
            "Validation data includes observations 4263 through 5683\n",
            "Training data includes observations 0 through 4262 joined with observations 5684 through the final observation.\n",
            "\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - 1s 8ms/step - loss: 758.3202 - mae: 758.1367 - val_loss: 721.6071 - val_mae: 721.4288\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 753.5457 - mae: 753.3730 - val_loss: 676.6812 - val_mae: 676.5152\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 745.8296 - mae: 745.6707 - val_loss: 637.3597 - val_mae: 637.2083\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 728.8231 - mae: 728.6783 - val_loss: 667.2124 - val_mae: 667.0744\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 691.8497 - mae: 691.7169 - val_loss: 615.5969 - val_mae: 615.4684\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 624.3486 - mae: 624.2228 - val_loss: 547.7358 - val_mae: 547.6121\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 532.2034 - mae: 532.0806 - val_loss: 515.2465 - val_mae: 515.1245\n",
            "Epoch 8/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 449.0098 - mae: 448.8883 - val_loss: 405.4079 - val_mae: 405.2858\n",
            "Epoch 9/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 404.9048 - mae: 404.7818 - val_loss: 373.6187 - val_mae: 373.4947\n",
            "Epoch 10/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 391.4426 - mae: 391.3174 - val_loss: 373.5984 - val_mae: 373.4715\n",
            "Epoch 11/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 384.2423 - mae: 384.1143 - val_loss: 372.5422 - val_mae: 372.4129\n",
            "Epoch 12/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 371.1628 - mae: 371.0316 - val_loss: 387.6872 - val_mae: 387.5549\n",
            "Epoch 13/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 344.9359 - mae: 344.8023 - val_loss: 510.8341 - val_mae: 510.6986\n",
            "Epoch 14/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 323.7998 - mae: 323.6625 - val_loss: 480.5355 - val_mae: 480.3965\n",
            "Epoch 15/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 307.3675 - mae: 307.2274 - val_loss: 413.6075 - val_mae: 413.4656\n",
            "Epoch 16/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 280.4401 - mae: 280.2967 - val_loss: 346.3376 - val_mae: 346.1934\n",
            "Epoch 17/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 268.8696 - mae: 268.7249 - val_loss: 280.4626 - val_mae: 280.3174\n",
            "Epoch 18/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 260.1736 - mae: 260.0275 - val_loss: 254.7961 - val_mae: 254.6493\n",
            "Epoch 19/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 252.2330 - mae: 252.0856 - val_loss: 239.0420 - val_mae: 238.8941\n",
            "Epoch 20/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 249.1234 - mae: 248.9750 - val_loss: 229.8649 - val_mae: 229.7159\n",
            "Epoch 21/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 244.5609 - mae: 244.4114 - val_loss: 228.1843 - val_mae: 228.0345\n",
            "Epoch 22/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.5363 - mae: 243.3864 - val_loss: 242.3194 - val_mae: 242.1693\n",
            "Epoch 23/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 238.9309 - mae: 238.7802 - val_loss: 225.2814 - val_mae: 225.1303\n",
            "Epoch 24/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 238.4840 - mae: 238.3322 - val_loss: 214.0907 - val_mae: 213.9384\n",
            "Epoch 25/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 240.9390 - mae: 240.7865 - val_loss: 212.6335 - val_mae: 212.4807\n",
            "Epoch 26/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 237.6851 - mae: 237.5324 - val_loss: 209.7839 - val_mae: 209.6308\n",
            "Epoch 27/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.8795 - mae: 233.7263 - val_loss: 206.9182 - val_mae: 206.7648\n",
            "Epoch 28/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.0833 - mae: 231.9295 - val_loss: 211.9318 - val_mae: 211.7777\n",
            "Epoch 29/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.0153 - mae: 232.8608 - val_loss: 206.2916 - val_mae: 206.1369\n",
            "Epoch 30/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 234.0811 - mae: 233.9262 - val_loss: 208.1994 - val_mae: 208.0443\n",
            "Epoch 31/150\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 230.9996 - mae: 230.8444 - val_loss: 206.2803 - val_mae: 206.1251\n",
            "Epoch 32/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.5636 - mae: 231.4083 - val_loss: 204.2424 - val_mae: 204.0870\n",
            "Epoch 33/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.3584 - mae: 228.2027 - val_loss: 207.3781 - val_mae: 207.2222\n",
            "Epoch 34/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.6555 - mae: 229.4994 - val_loss: 204.8755 - val_mae: 204.7191\n",
            "Epoch 35/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.4560 - mae: 231.2994 - val_loss: 206.3097 - val_mae: 206.1529\n",
            "Epoch 36/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.1954 - mae: 232.0385 - val_loss: 202.6664 - val_mae: 202.5093\n",
            "Epoch 37/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.9892 - mae: 228.8320 - val_loss: 203.0375 - val_mae: 202.8802\n",
            "Epoch 38/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.4664 - mae: 232.3086 - val_loss: 202.8686 - val_mae: 202.7105\n",
            "Epoch 39/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.8834 - mae: 225.7251 - val_loss: 204.7378 - val_mae: 204.5795\n",
            "Epoch 40/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.9140 - mae: 229.7556 - val_loss: 204.6521 - val_mae: 204.4935\n",
            "Epoch 41/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.8072 - mae: 226.6483 - val_loss: 203.4869 - val_mae: 203.3276\n",
            "Epoch 42/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.2726 - mae: 231.1129 - val_loss: 201.5298 - val_mae: 201.3701\n",
            "Epoch 43/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.9482 - mae: 224.7882 - val_loss: 202.4022 - val_mae: 202.2420\n",
            "Epoch 44/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.5636 - mae: 224.4035 - val_loss: 204.5390 - val_mae: 204.3785\n",
            "Epoch 45/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.1577 - mae: 228.9971 - val_loss: 201.8835 - val_mae: 201.7228\n",
            "Epoch 46/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.6340 - mae: 224.4730 - val_loss: 200.5646 - val_mae: 200.4033\n",
            "Epoch 47/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.7551 - mae: 225.5936 - val_loss: 201.3866 - val_mae: 201.2250\n",
            "Epoch 48/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.6572 - mae: 224.4954 - val_loss: 204.9527 - val_mae: 204.7907\n",
            "Epoch 49/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.1999 - mae: 223.0374 - val_loss: 201.9274 - val_mae: 201.7647\n",
            "Epoch 50/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.4410 - mae: 223.2781 - val_loss: 201.2020 - val_mae: 201.0389\n",
            "Epoch 51/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.1473 - mae: 226.9842 - val_loss: 202.2148 - val_mae: 202.0515\n",
            "Epoch 52/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.5669 - mae: 223.4034 - val_loss: 203.2359 - val_mae: 203.0723\n",
            "Epoch 53/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.6528 - mae: 224.4890 - val_loss: 200.7162 - val_mae: 200.5522\n",
            "Epoch 54/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.3061 - mae: 226.1418 - val_loss: 203.1386 - val_mae: 202.9742\n",
            "Epoch 55/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.2162 - mae: 224.0516 - val_loss: 201.8073 - val_mae: 201.6426\n",
            "Epoch 56/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.0215 - mae: 224.8569 - val_loss: 199.9703 - val_mae: 199.8056\n",
            "Epoch 57/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.7883 - mae: 223.6234 - val_loss: 201.0752 - val_mae: 200.9099\n",
            "Epoch 58/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.8361 - mae: 220.6709 - val_loss: 203.2309 - val_mae: 203.0654\n",
            "Epoch 59/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.8367 - mae: 221.6712 - val_loss: 201.3805 - val_mae: 201.2150\n",
            "Epoch 60/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.1486 - mae: 222.9829 - val_loss: 200.1392 - val_mae: 199.9732\n",
            "Epoch 61/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.3569 - mae: 224.1906 - val_loss: 202.3633 - val_mae: 202.1967\n",
            "Epoch 62/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.5831 - mae: 223.4161 - val_loss: 201.1941 - val_mae: 201.0268\n",
            "Epoch 63/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.4413 - mae: 222.2737 - val_loss: 200.8333 - val_mae: 200.6656\n",
            "Epoch 64/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.3896 - mae: 220.2215 - val_loss: 200.8102 - val_mae: 200.6417\n",
            "Epoch 65/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.7828 - mae: 220.6140 - val_loss: 200.3549 - val_mae: 200.1859\n",
            "Epoch 66/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.9273 - mae: 219.7581 - val_loss: 199.5217 - val_mae: 199.3524\n",
            "Epoch 67/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.0495 - mae: 218.8800 - val_loss: 199.2864 - val_mae: 199.1168\n",
            "Epoch 68/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.1978 - mae: 220.0280 - val_loss: 201.6140 - val_mae: 201.4440\n",
            "Epoch 69/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.9667 - mae: 220.7967 - val_loss: 199.3207 - val_mae: 199.1505\n",
            "Epoch 70/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.8840 - mae: 217.7135 - val_loss: 199.3956 - val_mae: 199.2249\n",
            "Epoch 71/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.4467 - mae: 222.2758 - val_loss: 199.9983 - val_mae: 199.8273\n",
            "Epoch 72/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.1918 - mae: 219.0207 - val_loss: 201.1628 - val_mae: 200.9915\n",
            "Epoch 73/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.2237 - mae: 217.0520 - val_loss: 200.6343 - val_mae: 200.4622\n",
            "Epoch 74/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.9546 - mae: 216.7821 - val_loss: 198.4547 - val_mae: 198.2824\n",
            "Epoch 75/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.3394 - mae: 219.1669 - val_loss: 198.8892 - val_mae: 198.7167\n",
            "Epoch 76/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.4457 - mae: 216.2727 - val_loss: 198.7562 - val_mae: 198.5829\n",
            "Epoch 77/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.3574 - mae: 219.1840 - val_loss: 199.5404 - val_mae: 199.3670\n",
            "Epoch 78/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.1366 - mae: 214.9630 - val_loss: 198.4370 - val_mae: 198.2634\n",
            "Epoch 79/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.8627 - mae: 220.6892 - val_loss: 198.8430 - val_mae: 198.6694\n",
            "Epoch 80/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.9965 - mae: 215.8227 - val_loss: 200.7605 - val_mae: 200.5865\n",
            "Epoch 81/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.9460 - mae: 214.7718 - val_loss: 199.6017 - val_mae: 199.4274\n",
            "Epoch 82/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.6077 - mae: 218.4331 - val_loss: 198.9562 - val_mae: 198.7813\n",
            "Epoch 83/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.8381 - mae: 214.6630 - val_loss: 199.9685 - val_mae: 199.7932\n",
            "Epoch 84/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.9463 - mae: 213.7708 - val_loss: 200.4109 - val_mae: 200.2351\n",
            "Epoch 85/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.4587 - mae: 215.2827 - val_loss: 199.5227 - val_mae: 199.3466\n",
            "Epoch 86/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.5231 - mae: 218.3470 - val_loss: 202.9163 - val_mae: 202.7400\n",
            "Epoch 87/150\n",
            "45/45 [==============================] - 0s 5ms/step - loss: 214.8145 - mae: 214.6380 - val_loss: 197.6382 - val_mae: 197.4616\n",
            "Epoch 88/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.6617 - mae: 213.4850 - val_loss: 199.1160 - val_mae: 198.9391\n",
            "Epoch 89/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.2137 - mae: 214.0367 - val_loss: 199.4547 - val_mae: 199.2776\n",
            "Epoch 90/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.0931 - mae: 213.9159 - val_loss: 200.3891 - val_mae: 200.2119\n",
            "Epoch 91/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.9918 - mae: 211.8141 - val_loss: 198.3400 - val_mae: 198.1621\n",
            "Epoch 92/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.9801 - mae: 213.8020 - val_loss: 198.0563 - val_mae: 197.8782\n",
            "Epoch 93/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.7264 - mae: 213.5480 - val_loss: 198.8848 - val_mae: 198.7062\n",
            "Epoch 94/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1182 - mae: 213.9395 - val_loss: 198.2876 - val_mae: 198.1088\n",
            "Epoch 95/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.9030 - mae: 212.7240 - val_loss: 199.3155 - val_mae: 199.1364\n",
            "Epoch 96/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.3018 - mae: 216.1223 - val_loss: 198.8874 - val_mae: 198.7077\n",
            "Epoch 97/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.8359 - mae: 215.6560 - val_loss: 199.8217 - val_mae: 199.6416\n",
            "Epoch 98/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.4192 - mae: 213.2387 - val_loss: 199.5586 - val_mae: 199.3781\n",
            "Epoch 99/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.9482 - mae: 212.7676 - val_loss: 197.3328 - val_mae: 197.1521\n",
            "Epoch 100/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.3207 - mae: 214.1399 - val_loss: 199.5377 - val_mae: 199.3569\n",
            "Epoch 101/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.3307 - mae: 216.1495 - val_loss: 199.9979 - val_mae: 199.8168\n",
            "Epoch 102/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.6104 - mae: 214.4292 - val_loss: 200.8127 - val_mae: 200.6314\n",
            "Epoch 103/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.3830 - mae: 212.2015 - val_loss: 198.1676 - val_mae: 197.9860\n",
            "Epoch 104/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1721 - mae: 213.9905 - val_loss: 198.0502 - val_mae: 197.8685\n",
            "Epoch 105/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.6110 - mae: 211.4291 - val_loss: 201.2580 - val_mae: 201.0763\n",
            "Epoch 106/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.5497 - mae: 213.3680 - val_loss: 200.9747 - val_mae: 200.7927\n",
            "Epoch 107/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.1011 - mae: 211.9191 - val_loss: 200.3200 - val_mae: 200.1378\n",
            "Epoch 108/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.8031 - mae: 212.6210 - val_loss: 202.0493 - val_mae: 201.8671\n",
            "Epoch 109/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.3423 - mae: 211.1600 - val_loss: 198.0148 - val_mae: 197.8325\n",
            "Epoch 110/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.3070 - mae: 214.1245 - val_loss: 197.9619 - val_mae: 197.7792\n",
            "Epoch 111/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.1798 - mae: 213.9969 - val_loss: 199.5295 - val_mae: 199.3463\n",
            "Epoch 112/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.3150 - mae: 213.1319 - val_loss: 197.7303 - val_mae: 197.5474\n",
            "Epoch 113/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.7166 - mae: 212.5335 - val_loss: 197.2505 - val_mae: 197.0674\n",
            "Epoch 114/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.8908 - mae: 211.7076 - val_loss: 197.6304 - val_mae: 197.4469\n",
            "Epoch 115/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.8555 - mae: 213.6719 - val_loss: 198.4775 - val_mae: 198.2937\n",
            "Epoch 116/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.2264 - mae: 213.0425 - val_loss: 197.6918 - val_mae: 197.5078\n",
            "Epoch 117/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.5590 - mae: 210.3747 - val_loss: 199.0097 - val_mae: 198.8253\n",
            "Epoch 118/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.8251 - mae: 212.6405 - val_loss: 200.8645 - val_mae: 200.6798\n",
            "Epoch 119/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.4155 - mae: 210.2309 - val_loss: 199.0281 - val_mae: 198.8430\n",
            "Epoch 120/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.6602 - mae: 210.4749 - val_loss: 198.2709 - val_mae: 198.0854\n",
            "Epoch 121/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.9053 - mae: 208.7196 - val_loss: 201.4236 - val_mae: 201.2375\n",
            "Epoch 122/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.5716 - mae: 210.3856 - val_loss: 200.5440 - val_mae: 200.3582\n",
            "Epoch 123/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.0276 - mae: 211.8416 - val_loss: 197.3611 - val_mae: 197.1748\n",
            "Epoch 124/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.3335 - mae: 214.1471 - val_loss: 198.2870 - val_mae: 198.1003\n",
            "Epoch 125/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.4968 - mae: 210.3098 - val_loss: 199.5401 - val_mae: 199.3525\n",
            "Epoch 126/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.9380 - mae: 211.7501 - val_loss: 198.1473 - val_mae: 197.9593\n",
            "Epoch 127/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4413 - mae: 214.2530 - val_loss: 200.1695 - val_mae: 199.9813\n",
            "Epoch 128/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.1530 - mae: 212.9645 - val_loss: 198.4546 - val_mae: 198.2660\n",
            "Epoch 129/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.3761 - mae: 213.1876 - val_loss: 199.3936 - val_mae: 199.2052\n",
            "Epoch 130/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.6314 - mae: 212.4429 - val_loss: 199.2174 - val_mae: 199.0288\n",
            "Epoch 131/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.9866 - mae: 209.7976 - val_loss: 199.6287 - val_mae: 199.4395\n",
            "Epoch 132/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.9524 - mae: 209.7630 - val_loss: 199.1574 - val_mae: 198.9678\n",
            "Epoch 133/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 207.2796 - mae: 207.0894 - val_loss: 199.6350 - val_mae: 199.4444\n",
            "Epoch 134/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.1927 - mae: 208.0022 - val_loss: 200.1283 - val_mae: 199.9378\n",
            "Epoch 135/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.4630 - mae: 209.2724 - val_loss: 198.9454 - val_mae: 198.7549\n",
            "Epoch 136/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.9040 - mae: 209.7132 - val_loss: 199.3049 - val_mae: 199.1139\n",
            "Epoch 137/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.6625 - mae: 209.4714 - val_loss: 199.4867 - val_mae: 199.2953\n",
            "Epoch 138/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.8700 - mae: 208.6785 - val_loss: 198.5890 - val_mae: 198.3974\n",
            "Epoch 139/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.4035 - mae: 208.2117 - val_loss: 200.0291 - val_mae: 199.8371\n",
            "Epoch 140/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.8802 - mae: 210.6882 - val_loss: 199.7507 - val_mae: 199.5590\n",
            "Epoch 141/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.0092 - mae: 210.8174 - val_loss: 199.2288 - val_mae: 199.0371\n",
            "Epoch 142/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.0219 - mae: 211.8301 - val_loss: 199.0525 - val_mae: 198.8606\n",
            "Epoch 143/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 207.4764 - mae: 207.2845 - val_loss: 198.2796 - val_mae: 198.0874\n",
            "Epoch 144/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.6585 - mae: 210.4661 - val_loss: 203.7283 - val_mae: 203.5355\n",
            "Epoch 145/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 209.1739 - mae: 208.9809 - val_loss: 197.0176 - val_mae: 196.8247\n",
            "Epoch 146/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 208.4942 - mae: 208.3012 - val_loss: 198.2309 - val_mae: 198.0377\n",
            "Epoch 147/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.2254 - mae: 210.0319 - val_loss: 201.4810 - val_mae: 201.2874\n",
            "Epoch 148/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.3440 - mae: 212.1501 - val_loss: 199.0601 - val_mae: 198.8661\n",
            "Epoch 149/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.2116 - mae: 212.0175 - val_loss: 199.8438 - val_mae: 199.6496\n",
            "Epoch 150/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.0795 - mae: 209.8851 - val_loss: 200.4013 - val_mae: 200.2066\n",
            "Processing fold #: 4\n",
            "Validation data includes observations 5684 through 7104\n",
            "Training data includes observations 0 through 5683 joined with observations 7105 through the final observation.\n",
            "\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - 1s 8ms/step - loss: 755.4280 - mae: 755.2558 - val_loss: 731.0328 - val_mae: 730.8668\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 749.7537 - mae: 749.5938 - val_loss: 692.2401 - val_mae: 692.0865\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 737.7990 - mae: 737.6512 - val_loss: 613.4201 - val_mae: 613.2786\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 710.4153 - mae: 710.2780 - val_loss: 497.5553 - val_mae: 497.4209\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 653.7764 - mae: 653.6431 - val_loss: 466.5244 - val_mae: 466.3918\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 564.8666 - mae: 564.7350 - val_loss: 597.5509 - val_mae: 597.4211\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 472.9006 - mae: 472.7713 - val_loss: 560.1383 - val_mae: 560.0096\n",
            "Epoch 8/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 416.5537 - mae: 416.4244 - val_loss: 533.8160 - val_mae: 533.6855\n",
            "Epoch 9/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 391.3090 - mae: 391.1769 - val_loss: 527.2850 - val_mae: 527.1516\n",
            "Epoch 10/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 372.6433 - mae: 372.5085 - val_loss: 525.6336 - val_mae: 525.4977\n",
            "Epoch 11/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 361.1977 - mae: 361.0612 - val_loss: 518.4288 - val_mae: 518.2921\n",
            "Epoch 12/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 347.4941 - mae: 347.3566 - val_loss: 496.1103 - val_mae: 495.9715\n",
            "Epoch 13/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 332.8908 - mae: 332.7515 - val_loss: 452.6570 - val_mae: 452.5169\n",
            "Epoch 14/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 323.0705 - mae: 322.9301 - val_loss: 428.4485 - val_mae: 428.3078\n",
            "Epoch 15/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 312.4742 - mae: 312.3330 - val_loss: 376.1105 - val_mae: 375.9688\n",
            "Epoch 16/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 306.1075 - mae: 305.9655 - val_loss: 351.3327 - val_mae: 351.1902\n",
            "Epoch 17/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 288.5969 - mae: 288.4541 - val_loss: 323.7575 - val_mae: 323.6144\n",
            "Epoch 18/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 276.9263 - mae: 276.7830 - val_loss: 315.5619 - val_mae: 315.4180\n",
            "Epoch 19/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 270.5337 - mae: 270.3892 - val_loss: 267.7848 - val_mae: 267.6398\n",
            "Epoch 20/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 257.8572 - mae: 257.7118 - val_loss: 242.7119 - val_mae: 242.5661\n",
            "Epoch 21/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 247.8077 - mae: 247.6611 - val_loss: 224.2480 - val_mae: 224.1014\n",
            "Epoch 22/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.4594 - mae: 243.3124 - val_loss: 229.4089 - val_mae: 229.2610\n",
            "Epoch 23/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.0812 - mae: 242.9332 - val_loss: 218.3200 - val_mae: 218.1719\n",
            "Epoch 24/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 243.7511 - mae: 243.6026 - val_loss: 213.0228 - val_mae: 212.8743\n",
            "Epoch 25/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 239.0761 - mae: 238.9271 - val_loss: 212.0193 - val_mae: 211.8700\n",
            "Epoch 26/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 238.5487 - mae: 238.3990 - val_loss: 208.3565 - val_mae: 208.2069\n",
            "Epoch 27/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 237.3342 - mae: 237.1844 - val_loss: 207.9860 - val_mae: 207.8356\n",
            "Epoch 28/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 238.1280 - mae: 237.9776 - val_loss: 206.1199 - val_mae: 205.9692\n",
            "Epoch 29/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 238.2059 - mae: 238.0546 - val_loss: 207.7743 - val_mae: 207.6225\n",
            "Epoch 30/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.5731 - mae: 233.4212 - val_loss: 205.8539 - val_mae: 205.7020\n",
            "Epoch 31/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 237.5679 - mae: 237.4157 - val_loss: 206.9717 - val_mae: 206.8191\n",
            "Epoch 32/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 235.4109 - mae: 235.2582 - val_loss: 208.4146 - val_mae: 208.2617\n",
            "Epoch 33/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.2494 - mae: 233.0964 - val_loss: 204.9017 - val_mae: 204.7486\n",
            "Epoch 34/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 235.2909 - mae: 235.1376 - val_loss: 205.7448 - val_mae: 205.5914\n",
            "Epoch 35/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 235.0041 - mae: 234.8506 - val_loss: 206.2158 - val_mae: 206.0621\n",
            "Epoch 36/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.8246 - mae: 233.6707 - val_loss: 207.2341 - val_mae: 207.0800\n",
            "Epoch 37/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.7796 - mae: 232.6253 - val_loss: 205.7931 - val_mae: 205.6387\n",
            "Epoch 38/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.5891 - mae: 233.4345 - val_loss: 207.0076 - val_mae: 206.8525\n",
            "Epoch 39/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.7785 - mae: 230.6235 - val_loss: 206.4420 - val_mae: 206.2870\n",
            "Epoch 40/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 233.5635 - mae: 233.4084 - val_loss: 208.1647 - val_mae: 208.0096\n",
            "Epoch 41/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.8529 - mae: 229.6977 - val_loss: 205.5561 - val_mae: 205.4006\n",
            "Epoch 42/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.1445 - mae: 230.9892 - val_loss: 205.0186 - val_mae: 204.8631\n",
            "Epoch 43/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.3661 - mae: 231.2105 - val_loss: 203.8824 - val_mae: 203.7268\n",
            "Epoch 44/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 231.2311 - mae: 231.0753 - val_loss: 206.1576 - val_mae: 206.0016\n",
            "Epoch 45/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.8049 - mae: 230.6487 - val_loss: 207.0629 - val_mae: 206.9062\n",
            "Epoch 46/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.9956 - mae: 228.8391 - val_loss: 206.9415 - val_mae: 206.7849\n",
            "Epoch 47/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.1328 - mae: 225.9758 - val_loss: 206.1327 - val_mae: 205.9758\n",
            "Epoch 48/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.7829 - mae: 228.6256 - val_loss: 207.1850 - val_mae: 207.0278\n",
            "Epoch 49/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.7029 - mae: 227.5454 - val_loss: 206.5797 - val_mae: 206.4220\n",
            "Epoch 50/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 232.3651 - mae: 232.2073 - val_loss: 203.5457 - val_mae: 203.3876\n",
            "Epoch 51/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.8708 - mae: 228.7126 - val_loss: 204.3959 - val_mae: 204.2373\n",
            "Epoch 52/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.8723 - mae: 226.7137 - val_loss: 205.1594 - val_mae: 205.0006\n",
            "Epoch 53/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.2842 - mae: 226.1252 - val_loss: 204.9948 - val_mae: 204.8357\n",
            "Epoch 54/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.2761 - mae: 223.1165 - val_loss: 203.2840 - val_mae: 203.1245\n",
            "Epoch 55/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.7478 - mae: 230.5880 - val_loss: 203.1700 - val_mae: 203.0103\n",
            "Epoch 56/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.9340 - mae: 226.7742 - val_loss: 206.8664 - val_mae: 206.7063\n",
            "Epoch 57/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 230.4647 - mae: 230.3043 - val_loss: 208.0016 - val_mae: 207.8412\n",
            "Epoch 58/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 229.1114 - mae: 228.9507 - val_loss: 203.7668 - val_mae: 203.6061\n",
            "Epoch 59/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 228.1828 - mae: 228.0220 - val_loss: 208.9172 - val_mae: 208.7566\n",
            "Epoch 60/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.0131 - mae: 226.8522 - val_loss: 202.7810 - val_mae: 202.6194\n",
            "Epoch 61/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.5375 - mae: 227.3759 - val_loss: 203.9561 - val_mae: 203.7944\n",
            "Epoch 62/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.0266 - mae: 224.8650 - val_loss: 202.4606 - val_mae: 202.2987\n",
            "Epoch 63/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.7509 - mae: 225.5891 - val_loss: 203.7574 - val_mae: 203.5956\n",
            "Epoch 64/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 227.7130 - mae: 227.5512 - val_loss: 203.5994 - val_mae: 203.4375\n",
            "Epoch 65/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.1429 - mae: 223.9809 - val_loss: 204.8028 - val_mae: 204.6406\n",
            "Epoch 66/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.0376 - mae: 224.8755 - val_loss: 205.0581 - val_mae: 204.8960\n",
            "Epoch 67/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.4459 - mae: 226.2836 - val_loss: 202.9565 - val_mae: 202.7941\n",
            "Epoch 68/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.0340 - mae: 223.8714 - val_loss: 201.9537 - val_mae: 201.7906\n",
            "Epoch 69/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.9886 - mae: 225.8255 - val_loss: 202.3913 - val_mae: 202.2281\n",
            "Epoch 70/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.7886 - mae: 225.6255 - val_loss: 201.9043 - val_mae: 201.7409\n",
            "Epoch 71/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.1484 - mae: 222.9848 - val_loss: 202.1246 - val_mae: 201.9611\n",
            "Epoch 72/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.1638 - mae: 226.0001 - val_loss: 204.0013 - val_mae: 203.8376\n",
            "Epoch 73/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 225.0556 - mae: 224.8917 - val_loss: 202.9186 - val_mae: 202.7544\n",
            "Epoch 74/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 223.5786 - mae: 223.4141 - val_loss: 207.0259 - val_mae: 206.8614\n",
            "Epoch 75/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.8742 - mae: 224.7097 - val_loss: 202.0493 - val_mae: 201.8846\n",
            "Epoch 76/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.5262 - mae: 224.3614 - val_loss: 205.6017 - val_mae: 205.4370\n",
            "Epoch 77/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.2380 - mae: 224.0732 - val_loss: 202.3904 - val_mae: 202.2252\n",
            "Epoch 78/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.5008 - mae: 224.3355 - val_loss: 202.1330 - val_mae: 201.9676\n",
            "Epoch 79/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.0442 - mae: 223.8787 - val_loss: 203.4779 - val_mae: 203.3123\n",
            "Epoch 80/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.3873 - mae: 222.2216 - val_loss: 200.9957 - val_mae: 200.8300\n",
            "Epoch 81/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 226.1689 - mae: 226.0028 - val_loss: 202.4118 - val_mae: 202.2455\n",
            "Epoch 82/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 224.4719 - mae: 224.3056 - val_loss: 201.9787 - val_mae: 201.8122\n",
            "Epoch 83/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.7791 - mae: 219.6128 - val_loss: 201.9699 - val_mae: 201.8036\n",
            "Epoch 84/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.8858 - mae: 221.7188 - val_loss: 201.1218 - val_mae: 200.9547\n",
            "Epoch 85/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.7970 - mae: 216.6296 - val_loss: 201.0618 - val_mae: 200.8943\n",
            "Epoch 86/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.8236 - mae: 221.6558 - val_loss: 201.9997 - val_mae: 201.8318\n",
            "Epoch 87/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.7856 - mae: 220.6174 - val_loss: 202.6705 - val_mae: 202.5020\n",
            "Epoch 88/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.9652 - mae: 220.7964 - val_loss: 203.2737 - val_mae: 203.1045\n",
            "Epoch 89/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.2025 - mae: 220.0332 - val_loss: 202.3609 - val_mae: 202.1913\n",
            "Epoch 90/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.8710 - mae: 219.7018 - val_loss: 201.4424 - val_mae: 201.2731\n",
            "Epoch 91/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.4033 - mae: 218.2337 - val_loss: 202.2085 - val_mae: 202.0388\n",
            "Epoch 92/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 221.9635 - mae: 221.7934 - val_loss: 200.9402 - val_mae: 200.7697\n",
            "Epoch 93/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.3845 - mae: 220.2144 - val_loss: 201.1354 - val_mae: 200.9652\n",
            "Epoch 94/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.9832 - mae: 222.8127 - val_loss: 202.2847 - val_mae: 202.1142\n",
            "Epoch 95/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.7386 - mae: 219.5679 - val_loss: 201.8559 - val_mae: 201.6850\n",
            "Epoch 96/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.0128 - mae: 218.8420 - val_loss: 203.9432 - val_mae: 203.7720\n",
            "Epoch 97/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 222.3416 - mae: 222.1703 - val_loss: 202.5184 - val_mae: 202.3469\n",
            "Epoch 98/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.2362 - mae: 217.0644 - val_loss: 202.3510 - val_mae: 202.1792\n",
            "Epoch 99/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.3216 - mae: 219.1495 - val_loss: 202.5643 - val_mae: 202.3924\n",
            "Epoch 100/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.0850 - mae: 217.9129 - val_loss: 204.6548 - val_mae: 204.4825\n",
            "Epoch 101/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.7533 - mae: 217.5806 - val_loss: 201.0277 - val_mae: 200.8549\n",
            "Epoch 102/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 220.2713 - mae: 220.0983 - val_loss: 203.1673 - val_mae: 202.9941\n",
            "Epoch 103/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4211 - mae: 214.2478 - val_loss: 200.0236 - val_mae: 199.8499\n",
            "Epoch 104/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.8917 - mae: 218.7176 - val_loss: 202.7267 - val_mae: 202.5521\n",
            "Epoch 105/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.5717 - mae: 217.3973 - val_loss: 202.4323 - val_mae: 202.2578\n",
            "Epoch 106/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.2047 - mae: 218.0302 - val_loss: 202.0457 - val_mae: 201.8710\n",
            "Epoch 107/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.0150 - mae: 217.8401 - val_loss: 202.0960 - val_mae: 201.9212\n",
            "Epoch 108/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.9020 - mae: 216.7272 - val_loss: 205.0036 - val_mae: 204.8286\n",
            "Epoch 109/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.9229 - mae: 216.7475 - val_loss: 202.5385 - val_mae: 202.3630\n",
            "Epoch 110/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.4285 - mae: 219.2526 - val_loss: 201.9977 - val_mae: 201.8217\n",
            "Epoch 111/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.9506 - mae: 214.7745 - val_loss: 200.2383 - val_mae: 200.0619\n",
            "Epoch 112/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.0701 - mae: 217.8935 - val_loss: 200.0227 - val_mae: 199.8459\n",
            "Epoch 113/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.9795 - mae: 218.8027 - val_loss: 201.2856 - val_mae: 201.1088\n",
            "Epoch 114/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 218.1465 - mae: 217.9697 - val_loss: 199.7249 - val_mae: 199.5480\n",
            "Epoch 115/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.3821 - mae: 214.2047 - val_loss: 200.4777 - val_mae: 200.3001\n",
            "Epoch 116/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.3973 - mae: 213.2197 - val_loss: 201.8153 - val_mae: 201.6373\n",
            "Epoch 117/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.7690 - mae: 215.5910 - val_loss: 202.1564 - val_mae: 201.9784\n",
            "Epoch 118/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 219.1248 - mae: 218.9465 - val_loss: 200.4535 - val_mae: 200.2751\n",
            "Epoch 119/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.0443 - mae: 215.8658 - val_loss: 203.9100 - val_mae: 203.7314\n",
            "Epoch 120/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 217.1212 - mae: 216.9424 - val_loss: 201.0886 - val_mae: 200.9101\n",
            "Epoch 121/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.5818 - mae: 216.4032 - val_loss: 201.0697 - val_mae: 200.8907\n",
            "Epoch 122/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.3335 - mae: 216.1546 - val_loss: 200.2642 - val_mae: 200.0852\n",
            "Epoch 123/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.9256 - mae: 212.7463 - val_loss: 200.4841 - val_mae: 200.3045\n",
            "Epoch 124/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.2372 - mae: 213.0576 - val_loss: 199.1004 - val_mae: 198.9209\n",
            "Epoch 125/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.9655 - mae: 212.7861 - val_loss: 201.2696 - val_mae: 201.0900\n",
            "Epoch 126/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4479 - mae: 214.2678 - val_loss: 201.1450 - val_mae: 200.9644\n",
            "Epoch 127/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.1283 - mae: 214.9477 - val_loss: 200.3908 - val_mae: 200.2100\n",
            "Epoch 128/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.1397 - mae: 215.9590 - val_loss: 200.0500 - val_mae: 199.8695\n",
            "Epoch 129/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4910 - mae: 214.3102 - val_loss: 200.2345 - val_mae: 200.0539\n",
            "Epoch 130/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.1815 - mae: 216.0005 - val_loss: 201.3922 - val_mae: 201.2108\n",
            "Epoch 131/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.2801 - mae: 214.0985 - val_loss: 201.6250 - val_mae: 201.4433\n",
            "Epoch 132/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.0527 - mae: 215.8710 - val_loss: 200.7061 - val_mae: 200.5244\n",
            "Epoch 133/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.0133 - mae: 214.8314 - val_loss: 201.4112 - val_mae: 201.2294\n",
            "Epoch 134/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.9324 - mae: 213.7505 - val_loss: 199.6086 - val_mae: 199.4265\n",
            "Epoch 135/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.6915 - mae: 214.5094 - val_loss: 200.9188 - val_mae: 200.7365\n",
            "Epoch 136/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.8730 - mae: 212.6905 - val_loss: 199.3342 - val_mae: 199.1515\n",
            "Epoch 137/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 215.1043 - mae: 214.9215 - val_loss: 204.6382 - val_mae: 204.4550\n",
            "Epoch 138/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.5238 - mae: 211.3407 - val_loss: 200.5877 - val_mae: 200.4044\n",
            "Epoch 139/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.3728 - mae: 212.1891 - val_loss: 199.8603 - val_mae: 199.6764\n",
            "Epoch 140/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.7895 - mae: 214.6054 - val_loss: 201.4335 - val_mae: 201.2491\n",
            "Epoch 141/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 211.9090 - mae: 211.7245 - val_loss: 199.4237 - val_mae: 199.2391\n",
            "Epoch 142/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 210.6214 - mae: 210.4366 - val_loss: 199.8107 - val_mae: 199.6258\n",
            "Epoch 143/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.4900 - mae: 214.3048 - val_loss: 198.4825 - val_mae: 198.2969\n",
            "Epoch 144/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 216.2048 - mae: 216.0190 - val_loss: 203.0362 - val_mae: 202.8505\n",
            "Epoch 145/150\n",
            "45/45 [==============================] - 0s 5ms/step - loss: 214.4903 - mae: 214.3043 - val_loss: 201.3178 - val_mae: 201.1315\n",
            "Epoch 146/150\n",
            "45/45 [==============================] - 0s 5ms/step - loss: 213.6087 - mae: 213.4224 - val_loss: 199.6126 - val_mae: 199.4262\n",
            "Epoch 147/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 213.7573 - mae: 213.5707 - val_loss: 199.2216 - val_mae: 199.0347\n",
            "Epoch 148/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.0861 - mae: 211.8991 - val_loss: 200.0036 - val_mae: 199.8165\n",
            "Epoch 149/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 212.0421 - mae: 211.8551 - val_loss: 199.5558 - val_mae: 199.3686\n",
            "Epoch 150/150\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 214.6945 - mae: 214.5070 - val_loss: 201.8691 - val_mae: 201.6813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_loss_history = [np.mean([x[i] for x in all_loss_history]) for i in range(num_epochs)]\n",
        "len(average_loss_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWz8s5QH2NaM",
        "outputId": "7a1baa2a-a9b0-4d14-8bf4-226c42f874af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_history]) for i in range(num_epochs)]\n",
        "len(average_val_loss_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7um5VCc2RK7",
        "outputId": "4a6948d3-1baa-4bb4-877c-104e15a5cf80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot model loss\n",
        "* Plot of average training loss and average validatoin loss across 5 folds.\n",
        "* I can see validation loss is flatenning after 100. Therefore, I will choose 100 epochs for the final model."
      ],
      "metadata": {
        "id": "XcD7R5-UG8sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of average training loss and average validatoin loss across 5 folds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(average_loss_history[:], label=\"Training loss\")\n",
        "plt.plot(average_val_loss_history[:], label=\"validation loss\")\n",
        "plt.title(\"Training and Validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "gq4fvj6kJYHK",
        "outputId": "619597e2-bdc9-4936-c7b3-5fdfc45c651c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Once You Choose the Final Configuration, Output that Model Here*"
      ],
      "metadata": {
        "id": "799Ss4r4cDB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final model\n",
        "model = build_model()\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=100, batch_size=128, verbose=1)\n",
        "model_evaluation = model.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "iZoiujQH9ARk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8fdd4d-bbb2-4289-a218-e8f81eccc604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "56/56 [==============================] - 1s 2ms/step - loss: 755.0975 - mae: 754.9078\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 748.5854 - mae: 748.4164\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 727.5020 - mae: 727.3558\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 663.6381 - mae: 663.5061\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 548.6181 - mae: 548.4933\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 441.5116 - mae: 441.3926\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 394.9410 - mae: 394.8218\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 367.4813 - mae: 367.3602\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 341.8980 - mae: 341.7745\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 326.5824 - mae: 326.4544\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 314.1124 - mae: 313.9817\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 307.1065 - mae: 306.9740\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 300.8475 - mae: 300.7136\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 293.6238 - mae: 293.4890\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 282.7855 - mae: 282.6499\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 271.5032 - mae: 271.3671\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 260.8763 - mae: 260.7396\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 255.3270 - mae: 255.1895\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 252.5345 - mae: 252.3962\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 248.0052 - mae: 247.8664\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 250.7135 - mae: 250.5738\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 245.6633 - mae: 245.5232\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 243.0522 - mae: 242.9115\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 243.0190 - mae: 242.8778\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 241.9104 - mae: 241.7688\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 239.4390 - mae: 239.2973\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 237.6458 - mae: 237.5046\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 234.2799 - mae: 234.1382\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 235.3880 - mae: 235.2461\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 233.5086 - mae: 233.3665\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 230.3449 - mae: 230.2021\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 231.1630 - mae: 231.0201\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 230.3439 - mae: 230.2007\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 230.2975 - mae: 230.1537\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 227.1635 - mae: 227.0193\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 226.3995 - mae: 226.2546\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 226.5583 - mae: 226.4128\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 227.3537 - mae: 227.2083\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 224.2737 - mae: 224.1278\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 225.5534 - mae: 225.4073\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 223.3569 - mae: 223.2109\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 223.8617 - mae: 223.7157\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 222.6511 - mae: 222.5047\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 223.9202 - mae: 223.7738\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 221.1194 - mae: 220.9725\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 220.8510 - mae: 220.7035\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 220.7663 - mae: 220.6184\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 219.8646 - mae: 219.7164\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 217.9970 - mae: 217.8487\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 221.1160 - mae: 220.9675\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 219.5311 - mae: 219.3820\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 219.6356 - mae: 219.4862\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 218.0187 - mae: 217.8688\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 218.9778 - mae: 218.8274\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 216.9479 - mae: 216.7969\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 218.0173 - mae: 217.8659\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 216.0277 - mae: 215.8758\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 216.3693 - mae: 216.2169\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 216.9385 - mae: 216.7858\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 217.3504 - mae: 217.1972\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 216.1779 - mae: 216.0242\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 213.9748 - mae: 213.8208\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 215.9983 - mae: 215.8440\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 216.7012 - mae: 216.5463\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 216.1086 - mae: 215.9531\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 214.9763 - mae: 214.8206\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.8294 - mae: 213.6733\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.5148 - mae: 213.3584\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 217.2719 - mae: 217.1151\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 213.3472 - mae: 213.1899\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 215.9827 - mae: 215.8250\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 213.0047 - mae: 212.8464\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 214.0071 - mae: 213.8486\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.8805 - mae: 213.7216\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 213.7501 - mae: 213.5910\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 214.2751 - mae: 214.1155\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.8510 - mae: 213.6909\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 214.4881 - mae: 214.3278\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 209.8376 - mae: 209.6769\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.1996 - mae: 213.0386\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 212.8428 - mae: 212.6814\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 211.6877 - mae: 211.5261\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 214.0405 - mae: 213.8784\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 211.2615 - mae: 211.0991\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.6324 - mae: 213.4698\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 213.1405 - mae: 212.9781\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.6378 - mae: 213.4750\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 213.5138 - mae: 213.3507\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 212.0003 - mae: 211.8369\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 210.9916 - mae: 210.8280\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 213.2959 - mae: 213.1323\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 210.7256 - mae: 210.5616\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 210.5885 - mae: 210.4240\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 211.6987 - mae: 211.5339\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 214.8569 - mae: 214.6917\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 211.3533 - mae: 211.1878\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 212.1147 - mae: 211.9490\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 207.5969 - mae: 207.4313\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 210.3721 - mae: 210.2063\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 206.8540 - mae: 206.6879\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 220.2693 - mae: 220.1029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "print(model_evaluation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqSJdZuP-OsF",
        "outputId": "d0a9cbe8-1a7d-4d8b-afbf-549271f6de3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220.269287109375, 220.10293579101562]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "7zNhP-9R9A1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at your model's final structure:"
      ],
      "metadata": {
        "id": "MFDAd_jXiFHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "XmZBB8Y4iJER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999c08dc-8b42-4bcc-ed11-5b7e89bbc66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_18 (Bat  (None, 11)               44        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 64)                768       \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,949\n",
            "Trainable params: 3,735\n",
            "Non-trainable params: 214\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "# I am going to upload my holdout dataset and pass it to your final model. This data will have the exact same set of columns as the data I gave you to fit your model to.\n",
        "uploaded = files.upload()\n",
        "bluebike_holdout = pd.read_csv(io.BytesIO(uploaded['bluebikes_holdout.csv']))\n",
        "\n",
        "# I'm then going to pre-process it using your data processing function.\n",
        "holdout_predictors, holdout_labels = processData(bluebike_holdout)\n",
        "\n",
        "# Then, I'm going to request your model's prediction performance. \n",
        "# I expect you should be able to predict trip durations within at least 1000 seconds of ground truth, on average (at worst).\n",
        "loss_metrics = model.evaluate(holdout_predictors,holdout_labels,verbose=1)"
      ],
      "metadata": {
        "id": "RzEJ5p5tS4Zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}